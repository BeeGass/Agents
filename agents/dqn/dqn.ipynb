{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Torch Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as T\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Gym Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A.L.E: Arcade Learning Environment (version 0.7.4+069f8bd)\n",
      "[Powered by Stella]\n"
     ]
    }
   ],
   "source": [
    "from ale_py import ALEInterface\n",
    "ale = ALEInterface()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Game console created:\n",
      "  ROM file:  /home/beegass/.virtualenvs/dl_1/lib/python3.8/site-packages/AutoROM/roms/breakout.bin\n",
      "  Cart Name: Breakout - Breakaway IV (1978) (Atari)\n",
      "  Cart MD5:  f34f08e5eb96e500e851a80be3277a56\n",
      "  Display Format:  AUTO-DETECT ==> NTSC\n",
      "  ROM Size:        2048\n",
      "  Bankswitch Type: AUTO-DETECT ==> 2K\n",
      "\n",
      "Running ROM file...\n",
      "Random seed is 1649384338\n"
     ]
    }
   ],
   "source": [
    "from ale_py.roms import Breakout\n",
    "ale.loadROM(Breakout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym.wrappers import FrameStack\n",
    "from gym.wrappers.monitoring.video_recorder import VideoRecorder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### All Other Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import trange\n",
    "import random\n",
    "import copy\n",
    "from loguru import logger\n",
    "import wandb\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbeegass\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/beegass/.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login(key=\"5966d774f384473f3d7ed674ef762b1a26a54d63\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") # check if GPU is available"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The Meat And Potatoes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gym_Env():\n",
    "    def __init__(self, env_name, max_steps=1000, max_episodes=10000):\n",
    "        self.le_env = FrameStack(gym.make(env_name, render_mode='rgb_array'), 4)\n",
    "        self.max_steps = max_steps\n",
    "        self.max_episodes = max_episodes\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Replay_Buffer():\n",
    "    def __init__(self, capacity, mini_batch_size=128):\n",
    "        self.rb = []\n",
    "        self.capacity = capacity\n",
    "        self.mini_batch_size = mini_batch_size\n",
    "        self.current_batch = None\n",
    "\n",
    "    def get_rb_batch(self):\n",
    "        sample = random.sample(self.rb, self.mini_batch_size)\n",
    "        states, actions, next_states, rewards, done = zip(*sample[0: (self.mini_batch_size - 1)])\n",
    "        return states, actions, next_states, rewards, done\n",
    "    \n",
    "    def add_to_rb(self, new_transition):\n",
    "        if len(self.rb) >= self.capacity:\n",
    "            del self.rb[0] \n",
    "        self.rb.append(new_transition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Deep_Q_Network(nn.Module):\n",
    "    def __init__(self, in_channels=4, num_actions=4):\n",
    "        super(Deep_Q_Network, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, 32, kernel_size=8, stride=4)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
    "        self.fc4 = nn.Linear(7 * 7 * 64, 512)\n",
    "        self.fc5 = nn.Linear(512, num_actions)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.fc4(x.view(x.size(0), -1)))\n",
    "        return self.fc5(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(nn.Module):\n",
    "    def __init__(self, pred_model):\n",
    "        super(Agent, self).__init__()\n",
    "        self.agent = pred_model\n",
    "        self.target = None\n",
    "        \n",
    "    def agent_policy(self, state, pred_model=True, grad=False):\n",
    "        q_val = None\n",
    "        state = state.to(device)\n",
    "        if pred_model:\n",
    "            if grad:\n",
    "                q_val = self.agent(state)\n",
    "            else:\n",
    "                with torch.no_grad():\n",
    "                    q_val = self.agent(state)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                q_val = self.target(state)\n",
    "        return q_val\n",
    "    \n",
    "    def copy_pred_to_target(self):\n",
    "        self.target = copy.deepcopy(self.agent)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy(environment, the_agent, state, step_count, epsilon=0.1):\n",
    "    prob = random.random()\n",
    "    action = environment.le_env.action_space.sample() # pick action from action space\n",
    "    if prob < 1 - epsilon:\n",
    "        q_val = the_agent.agent_policy(state) # retrieve best action, based off its action-value \n",
    "        action = torch.argmax(q_val)\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_decay(environment, episode_num, p_init=1, p_end=0.1, epsilon_decay=200):\n",
    "    current_episode_rate = (environment.max_episodes - episode_num) / environment.max_episodes\n",
    "    epsilon_decay_rate = math.exp(-1. * episode_num / epsilon_decay) #max(current_episode_rate, 0)\n",
    "    return ((p_init - p_end) * (epsilon_decay_rate)) + p_end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(state):\n",
    "    \n",
    "    # convert state to numpy array and then to torch tensor\n",
    "    frame = torch.from_numpy(np.array(state).astype(np.float32))\n",
    "    \n",
    "    # reshape so that grayscaling is possible\n",
    "    reshaped_frame = frame.reshape(4, 3, 210, 160)\n",
    "    \n",
    "    # grayscale image\n",
    "    gray_frame = T.Grayscale()(reshaped_frame)\n",
    "    \n",
    "    # reshape image so network can process it\n",
    "    reshaped_gray_frame = gray_frame.reshape(1, 4, 210, 160)\n",
    "    \n",
    "    # downscale image to 84x84\n",
    "    small_gray_frame = T.Resize((84, 84))(reshaped_gray_frame)\n",
    "    \n",
    "    return small_gray_frame\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def config_tune() -> None:\n",
    "    cfg = {\n",
    "        'method': 'bayes', #grid, random\n",
    "        'metric': {\n",
    "            'name': 'total mean episodic reward',\n",
    "            'goal': 'maximize'   \n",
    "        },\n",
    "        'parameters': {\n",
    "            'lr': {'distribution': 'uniform',\n",
    "                                      'min': 0.000001,\n",
    "                                      'max': 1},\n",
    "            'weight_decay': {'distribution': 'uniform',\n",
    "                                      'min': 0.00001,\n",
    "                                      'max': 1},\n",
    "            'replay_buffer_size': {'distribution': 'int_uniform',\n",
    "                                                'min': 10000,\n",
    "                                                'max': 30000},\n",
    "            'gamma': {'distribution': 'uniform',\n",
    "                                      'min': 0.1,\n",
    "                                      'max': 1},\n",
    "            'delta': {'distribution': 'uniform',\n",
    "                                      'min': 1,\n",
    "                                      'max': 10},\n",
    "            'target_freq': {'distribution': 'int_uniform',\n",
    "                                      'min': 1,\n",
    "                                      'max': 400},\n",
    "            'p_end': {'distribution': 'uniform',\n",
    "                                      'min': 0.0001,\n",
    "                                      'max': 0.1},\n",
    "            'epsilon_decay_rate': {'distribution': 'int_uniform',\n",
    "                                      'min': 1,\n",
    "                                      'max': 500},\n",
    "            'batch_size': {\n",
    "                'value': 64\n",
    "            },\n",
    "            'max_episodes': {\n",
    "                'value': 10000 \n",
    "            },\n",
    "            'max_steps': {\n",
    "                'value': 100000\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vanilla_config() -> None:\n",
    "    cfg = {\n",
    "        'batch_size': 64,\n",
    " \t    'delta': 3.38,\n",
    "        'gamma': 0.9981,\n",
    "        'lr': 0.4053,\n",
    "        'max_episodes': 10000,\n",
    "        'max_steps': 100000,\n",
    "        'p_end': 0.03918,\n",
    "        'replay_buffer_size': 13497,\n",
    "        'target_freq': 460,\n",
    "        'weight_decay': 0.4424,\n",
    "    }\n",
    "    \n",
    "    return cfg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Buidling Optimizers and Schedulers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_optimizer(model, optimizer_name='adam', learning_rate=0.01, weight_decay=0.01, momentum=0.9):\n",
    "    try:\n",
    "        optimizer = None\n",
    "        if optimizer_name == \"sgd\":\n",
    "            optimizer = optim.SGD(model.parameters(), \n",
    "                                  lr=learning_rate, \n",
    "                                  momentum=momentum)\n",
    "            \n",
    "        elif optimizer_name == \"adam\":\n",
    "            optimizer = optim.Adam(model.parameters(), \n",
    "                                   lr=learning_rate, \n",
    "                                   weight_decay=weight_decay)\n",
    "               \n",
    "        return optimizer\n",
    "    except:\n",
    "        print(\"Error: Invalid optimizer specified.\")\n",
    "        sys.exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_scheduler(optimizer, sched_name='reduce_lr', patience=5, verbose=True):\n",
    "    try: \n",
    "        sched = None\n",
    "        if sched_name == \"reduce_lr\":\n",
    "            sched = optim.lr_scheduler.ReduceLROnPlateau(optimizer, \n",
    "                                                         mode='min', \n",
    "                                                         patience=patience, \n",
    "                                                         verbose=verbose)\n",
    "        elif sched_name == 'TODO':\n",
    "            pass\n",
    "            #TODO: add other scheduler\n",
    "            \n",
    "        return sched\n",
    "    except:\n",
    "        logger.error(\"Error: Invalid scheduler specified.\")\n",
    "        sys.exit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(step_count, episode_tuple, replay_buffer, the_agent, loss_fn, optimizer, scheduler, gamma=0.95):\n",
    "    #1. copy weights from prediction network to target target network\n",
    "    (episode_num, target_freq) = episode_tuple\n",
    "    if the_agent.target is not None:\n",
    "        if step_count % target_freq == 0:\n",
    "            the_agent.copy_pred_to_target() \n",
    "    else:\n",
    "        the_agent.copy_pred_to_target()\n",
    "    \n",
    "    #2. retrieve (s, a, r, s') from mini_batch\n",
    "    states, actions, next_states, rewards, done = replay_buffer.get_rb_batch()\n",
    "    \n",
    "    states = torch.vstack(states)\n",
    "    actions = torch.tensor(actions, dtype = torch.long, device = device).detach()\n",
    "    next_states = torch.vstack(next_states)\n",
    "    rewards = torch.tensor(rewards, dtype = torch.float, device = device)\n",
    "    done = torch.tensor(done, dtype=torch.int32, device = device)\n",
    "    \n",
    "    pred_q_val_matrix = the_agent.agent_policy(states, grad=True)\n",
    "    pred_q_val = torch.gather(pred_q_val_matrix, 1, actions.unsqueeze(1)).squeeze(1)\n",
    "    target_q_val_matrix = the_agent.agent_policy(next_states, pred_model=False)\n",
    "    target_q_val = torch.max(target_q_val_matrix, dim=1)[0]\n",
    "    #zero_or_one = torch.ones(done.shape, dtype=torch.float, device = device) - done\n",
    "    y_j = rewards + ((gamma * target_q_val) * (1 - done))\n",
    "    \n",
    "    output = loss_fn(pred_q_val, y_j.detach())\n",
    "    optimizer.zero_grad()\n",
    "    output.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Episode Specific Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode(video, render, episode_tuple, environment, the_agent, replay_buffer, state, epsilon, gamma, loss_fn, optimizer, scheduler):\n",
    "    step_count = 0\n",
    "    cumulative_reward = 0\n",
    "    while True:\n",
    "        \n",
    "        #0. either explore or exploit\n",
    "        action = epsilon_greedy(environment=environment,\n",
    "                                the_agent=the_agent,\n",
    "                                state=state,\n",
    "                                step_count=step_count,\n",
    "                                epsilon=epsilon)\n",
    "        \n",
    "        #1. render environment only when flag is set to True\n",
    "        if render:\n",
    "            video.capture_frame()\n",
    "        \n",
    "        #2. pass action to environment\n",
    "        (next_state, reward, done, metadata) = environment.le_env.step(action)\n",
    "        \n",
    "        #3. get s' back from environment and preprocess (s' -> preprocessed_s')\n",
    "        preprocessed_next_state = preprocess(next_state)\n",
    "        \n",
    "        #4. add transition (s, a, s', r) to replay buffer\n",
    "        replay_buffer.add_to_rb((state, action, preprocessed_next_state, reward, done))\n",
    "        \n",
    "        #5. if replay buffer is full, sample mini batch and update model\n",
    "        if len(replay_buffer.rb) > replay_buffer.mini_batch_size and not epsilon <= 0.000001 and step_count % 4 == 0:\n",
    "            train(step_count, episode_tuple, replay_buffer, the_agent, loss_fn, optimizer, scheduler, gamma)\n",
    "            \n",
    "        \n",
    "        cumulative_reward += reward\n",
    "        \n",
    "        #6. check max number of time steps has been reached or if game is complete\n",
    "        if step_count >= environment.max_steps or done:\n",
    "            step_count += 1\n",
    "            return cumulative_reward, step_count, video\n",
    "        \n",
    "        state = preprocessed_next_state\n",
    "        \n",
    "        step_count += 1\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Episodic Loop  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run():\n",
    "    cfg = vanilla_config()\n",
    "    with wandb.init(project=\"BeeGass-Agents\", entity=\"beegass\", config=cfg, monitor_gym=True):\n",
    "        config = wandb.config\n",
    "        \n",
    "        # initialize gym environment\n",
    "        environment = Gym_Env(env_name='ALE/Breakout-v5', max_steps=config.max_steps, max_episodes=config.max_episodes)\n",
    "        \n",
    "        # initialize prediction network\n",
    "        #pred_net = Deep_Q_Network(environment.le_env.action_space.n).to(device)\n",
    "        pred_net = Deep_Q_Network().to(device)\n",
    "        \n",
    "        # initialize agent that contains both prediction network and target network\n",
    "        the_agent = Agent(pred_model=pred_net)\n",
    "        \n",
    "        # define loss function\n",
    "        loss_fn = nn.HuberLoss(reduction='mean', delta=config.delta)\n",
    "        \n",
    "        # define optimizer\n",
    "        optimizer = build_optimizer(model=the_agent.agent, \n",
    "                                    optimizer_name='adam', \n",
    "                                    learning_rate=config.lr, \n",
    "                                    weight_decay=config.weight_decay)\n",
    "        \n",
    "        # define scheduler\n",
    "        scheduler = build_scheduler(optimizer, \n",
    "                                    sched_name='reduce_lr', \n",
    "                                    patience=5, \n",
    "                                    verbose=True)\n",
    "        \n",
    "        # initialize replay buffer\n",
    "        replay_buffer = Replay_Buffer(capacity=config.replay_buffer_size, mini_batch_size=64)\n",
    "        \n",
    "        render_flag = False\n",
    "        epsilon = 1\n",
    "        episode_cumulative_reward = 0\n",
    "        total_steps = 0\n",
    "        for e in range(environment.max_episodes):\n",
    "            video = None\n",
    "            if e + 1 % 10000 == 0:\n",
    "                render_flag = True\n",
    "                video = VideoRecorder(environment.le_env, f\"./videos/breakout/episode{e}.mp4\", enabled=True)\n",
    "            else:\n",
    "                render_flag = False\n",
    "            \n",
    "            # 0. get initial state, s_{0}, and preprocess it (s_{0} -> preprocessed_s)\n",
    "            state = environment.le_env.reset()\n",
    "            \n",
    "            # 0.1: preprocess state\n",
    "            preprocessed_state = preprocess(state)\n",
    "            \n",
    "            # 1. iterate over steps in episode\n",
    "            cumulative_reward, step_count, video = run_episode(video=video,\n",
    "                                                render=render_flag,\n",
    "                                                environment=environment,\n",
    "                                                episode_tuple=(e, config.target_freq), \n",
    "                                                the_agent=the_agent,\n",
    "                                                replay_buffer=replay_buffer,\n",
    "                                                state=preprocessed_state, \n",
    "                                                epsilon=epsilon,\n",
    "                                                gamma=config.gamma,\n",
    "                                                loss_fn=loss_fn,\n",
    "                                                optimizer=optimizer, \n",
    "                                                scheduler=scheduler)\n",
    "            \n",
    "            episode_cumulative_reward += cumulative_reward\n",
    "            total_steps += step_count\n",
    "            wandb.log({\"episode\": e, \"total mean episodic reward\": (episode_cumulative_reward/(e+1))}, step=e)\n",
    "            wandb.log({\"episode\": e, \"reward per episode\": cumulative_reward}, step=e)\n",
    "            wandb.log({\"episode\": e, \"step_count\": step_count}, step=e)\n",
    "            wandb.log({\"episode\": e, \"total_step_count\": total_steps}, step=e)\n",
    "            wandb.log({\"step_count\": step_count, \"mean step reward\": cumulative_reward/step_count}, step=e)\n",
    "            wandb.log({\"total_steps\": total_steps, \"total mean episodic-step reward\": episode_cumulative_reward/total_steps}, step=e)\n",
    "            \n",
    "            if render_flag:\n",
    "                wandb.log({\"video\": wandb.Video(f\"./videos/breakout/episode{e}.mp4\", fps=4, format=\"mp4\")})\n",
    "                video.close\n",
    "            \n",
    "            # 3. decay epsilon\n",
    "            # epsilon = config.decay_rate * epsilon\n",
    "            epsilon = epsilon_decay(environment, e+1, 1, config.p_end, config.epsilon_decay_rate)\n",
    "            wandb.log({\"episode\": e, \"epsilon\": epsilon}, step=e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: 150nu0fs\n",
      "Sweep URL: https://wandb.ai/beegass/dqn-sweeps/sweeps/150nu0fs\n"
     ]
    }
   ],
   "source": [
    "sweep_id = wandb.sweep(config_tune(), project=\"dqn-sweeps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: bnx6uwfi with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdelta: 9.153932572597787\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepsilon_decay_rate: 154\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tgamma: 0.5948346340403956\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.9748646593026162\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmax_episodes: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmax_steps: 100000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tp_end: 0.07039231345936024\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \treplay_buffer_size: 26436\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttarget_freq: 289\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.9643052328135352\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg entity when running a sweep.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.12.13 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/beegass/Documents/Coding/Agents/agents/dqn/wandb/run-20220407_221902-bnx6uwfi</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/beegass/dqn-sweeps/runs/bnx6uwfi\" target=\"_blank\">fearless-sweep-1</a></strong> to <a href=\"https://wandb.ai/beegass/dqn-sweeps\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/beegass/dqn-sweeps/sweeps/150nu0fs\" target=\"_blank\">https://wandb.ai/beegass/dqn-sweeps/sweeps/150nu0fs</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A.L.E: Arcade Learning Environment (version 0.7.4+069f8bd)\n",
      "[Powered by Stella]\n",
      "/home/beegass/.virtualenvs/dl_1/lib/python3.8/site-packages/gym/utils/seeding.py:138: DeprecationWarning: \u001b[33mWARN: Function `hash_seed(seed, max_bytes)` is marked as deprecated and will be removed in the future. \u001b[0m\n",
      "  deprecation(\n",
      "/home/beegass/.virtualenvs/dl_1/lib/python3.8/site-packages/gym/utils/seeding.py:175: DeprecationWarning: \u001b[33mWARN: Function `_bigint_from_bytes(bytes)` is marked as deprecated and will be removed in the future. \u001b[0m\n",
      "  deprecation(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1dd15ef18736420ebc241612ea354f21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>episode</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>epsilon</td><td>█▄▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>mean step reward</td><td>▃▃▄▁▃▃▄▅▁▄▇▆▅▁▇▆▃▆▇▇▅▄▆▇▅▄▃▅▁▃▅▃▁▄▅▃█▆▅▅</td></tr><tr><td>reward per episode</td><td>▃▃▅▁▃▃▃▅▁▃▆▅▅▁▆▅▃▆█▆▅▃▆▆▅▅▃▆▁▃▆▃▁▃▅▃█▆▅▅</td></tr><tr><td>step_count</td><td>▃▃▆▃▇▃▃▅▂▃▅▄▄▁▄▃▄▇▇▅▄▃▅▄▄█▄▇▃▅▇▄▃▃▄▄▆▆▅▅</td></tr><tr><td>total mean episodic reward</td><td>▁▁▄▅▅▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇████████████████████</td></tr><tr><td>total mean episodic-step reward</td><td>▄▄▅▂▁▂▃▃▄▄▅▆▆▇██▇▇▇▇▆▆▆▆▆▆▆▆▆▅▅▅▅▄▄▄▄▄▄▄</td></tr><tr><td>total_step_count</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>total_steps</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇███</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>episode</td><td>9999</td></tr><tr><td>epsilon</td><td>0.07039</td></tr><tr><td>mean step reward</td><td>0.00881</td></tr><tr><td>reward per episode</td><td>2.0</td></tr><tr><td>step_count</td><td>227</td></tr><tr><td>total mean episodic reward</td><td>2.0131</td></tr><tr><td>total mean episodic-step reward</td><td>0.00769</td></tr><tr><td>total_step_count</td><td>2617103</td></tr><tr><td>total_steps</td><td>2617103</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">fearless-sweep-1</strong>: <a href=\"https://wandb.ai/beegass/dqn-sweeps/runs/bnx6uwfi\" target=\"_blank\">https://wandb.ai/beegass/dqn-sweeps/runs/bnx6uwfi</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220407_221902-bnx6uwfi/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: uhehir3l with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdelta: 9.437649115394349\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepsilon_decay_rate: 65\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tgamma: 0.2603168585667468\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.4618299117629389\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmax_episodes: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmax_steps: 100000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tp_end: 0.029956521457085952\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \treplay_buffer_size: 23180\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttarget_freq: 210\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.6433205218738568\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg entity when running a sweep.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.12.13 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/beegass/Documents/Coding/Agents/agents/dqn/wandb/run-20220408_004637-uhehir3l</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/beegass/dqn-sweeps/runs/uhehir3l\" target=\"_blank\">vibrant-sweep-2</a></strong> to <a href=\"https://wandb.ai/beegass/dqn-sweeps\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/beegass/dqn-sweeps/sweeps/150nu0fs\" target=\"_blank\">https://wandb.ai/beegass/dqn-sweeps/sweeps/150nu0fs</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/beegass/.virtualenvs/dl_1/lib/python3.8/site-packages/gym/utils/seeding.py:138: DeprecationWarning: \u001b[33mWARN: Function `hash_seed(seed, max_bytes)` is marked as deprecated and will be removed in the future. \u001b[0m\n",
      "  deprecation(\n",
      "/home/beegass/.virtualenvs/dl_1/lib/python3.8/site-packages/gym/utils/seeding.py:175: DeprecationWarning: \u001b[33mWARN: Function `_bigint_from_bytes(bytes)` is marked as deprecated and will be removed in the future. \u001b[0m\n",
      "  deprecation(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a092c16797cc4781b22b155357d88cd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>episode</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>epsilon</td><td>█▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>mean step reward</td><td>▅▅▁▄▄▃▅▃▅▅█▁▂▃▃▃▂▂▂▄▄▁▃▁▆▆▄▄▅▄▅▃▃▃▅▃▄▁▄▄</td></tr><tr><td>reward per episode</td><td>▄▄▁▃▃▂▄▂▅▄█▁▂▂▃▃▂▂▂▄▄▂▂▁▅▅▃▃▅▃▄▂▂▃▄▃▅▂▄▃</td></tr><tr><td>step_count</td><td>▂▂▁▂▂▁▂▁▃▂▃▂▂▂▂▂▂▂▃▂▃▆▂▁▂▃▁▂▃▁▂▁▂▂▂▃▄█▃▂</td></tr><tr><td>total mean episodic reward</td><td>▁▂▃▂▃▃▆▆▇███▇▇▇▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▆▆▆▆▆▆▆▆</td></tr><tr><td>total mean episodic-step reward</td><td>▄▅▄▁▂▂▆▇███▇▆▅▅▅▄▃▃▃▃▃▃▄▄▄▄▄▄▄▄▅▄▄▄▄▄▃▃▃</td></tr><tr><td>total_step_count</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>total_steps</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇███</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>episode</td><td>9999</td></tr><tr><td>epsilon</td><td>0.02996</td></tr><tr><td>mean step reward</td><td>0.00851</td></tr><tr><td>reward per episode</td><td>2.0</td></tr><tr><td>step_count</td><td>235</td></tr><tr><td>total mean episodic reward</td><td>2.2789</td></tr><tr><td>total mean episodic-step reward</td><td>0.00835</td></tr><tr><td>total_step_count</td><td>2727841</td></tr><tr><td>total_steps</td><td>2727841</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">vibrant-sweep-2</strong>: <a href=\"https://wandb.ai/beegass/dqn-sweeps/runs/uhehir3l\" target=\"_blank\">https://wandb.ai/beegass/dqn-sweeps/runs/uhehir3l</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220408_004637-uhehir3l/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: b95zap60 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdelta: 7.704582656702691\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepsilon_decay_rate: 319\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tgamma: 0.38698952798927744\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.9075937762257816\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmax_episodes: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmax_steps: 100000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tp_end: 0.012115378436975244\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \treplay_buffer_size: 26200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttarget_freq: 332\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.20353263459017695\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg entity when running a sweep.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.12.13 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/beegass/Documents/Coding/Agents/agents/dqn/wandb/run-20220408_032204-b95zap60</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/beegass/dqn-sweeps/runs/b95zap60\" target=\"_blank\">solar-sweep-3</a></strong> to <a href=\"https://wandb.ai/beegass/dqn-sweeps\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/beegass/dqn-sweeps/sweeps/150nu0fs\" target=\"_blank\">https://wandb.ai/beegass/dqn-sweeps/sweeps/150nu0fs</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/beegass/.virtualenvs/dl_1/lib/python3.8/site-packages/gym/utils/seeding.py:138: DeprecationWarning: \u001b[33mWARN: Function `hash_seed(seed, max_bytes)` is marked as deprecated and will be removed in the future. \u001b[0m\n",
      "  deprecation(\n",
      "/home/beegass/.virtualenvs/dl_1/lib/python3.8/site-packages/gym/utils/seeding.py:175: DeprecationWarning: \u001b[33mWARN: Function `_bigint_from_bytes(bytes)` is marked as deprecated and will be removed in the future. \u001b[0m\n",
      "  deprecation(\n",
      "wandb: Network error (ReadTimeout), entering retry loop.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48dddd04103c4573a68dacb92c9d94ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>episode</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>epsilon</td><td>█▆▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>mean step reward</td><td>▁▄▅▄▃▅▃▅▃▄▃▃▄█▃▁▄▁▁▃▃▃▄▅▁▂▁▅▁▅▁▂▄▁▅▅▃▃▃▄</td></tr><tr><td>reward per episode</td><td>▁▃▄▃▃▄▃▄▂▄▃▃▃█▃▁▄▁▁▂▂▂▃▄▁▂▁▄▁▅▁▂▃▁▆▄▂▂▂▃</td></tr><tr><td>step_count</td><td>▁▃▅▄▅▅▇▅▂▇▇▅▄▆▅▃▆▁▁▃▂▃▄▄▂▄▃▄▄▆▁▅▃▅█▄▂▂▃▃</td></tr><tr><td>total mean episodic reward</td><td>▁▂▄▄▄▄▅▅▆▆▆▇▇███████████████████████████</td></tr><tr><td>total mean episodic-step reward</td><td>▃▄█▅▂▃▂▂▃▁▁▁▂▃▄▃▃▃▃▄▄▄▄▅▄▄▄▃▃▃▃▂▂▂▂▂▂▂▃▃</td></tr><tr><td>total_step_count</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>total_steps</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>episode</td><td>9999</td></tr><tr><td>epsilon</td><td>0.01212</td></tr><tr><td>mean step reward</td><td>0.00546</td></tr><tr><td>reward per episode</td><td>1.0</td></tr><tr><td>step_count</td><td>183</td></tr><tr><td>total mean episodic reward</td><td>2.1255</td></tr><tr><td>total mean episodic-step reward</td><td>0.00794</td></tr><tr><td>total_step_count</td><td>2676005</td></tr><tr><td>total_steps</td><td>2676005</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">solar-sweep-3</strong>: <a href=\"https://wandb.ai/beegass/dqn-sweeps/runs/b95zap60\" target=\"_blank\">https://wandb.ai/beegass/dqn-sweeps/runs/b95zap60</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220408_032204-b95zap60/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: z5v1x87a with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdelta: 6.1976357247085705\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepsilon_decay_rate: 212\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tgamma: 0.8623963865483852\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.2164533292269944\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmax_episodes: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmax_steps: 100000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tp_end: 0.05449872593742215\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \treplay_buffer_size: 17535\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttarget_freq: 250\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.3678578503199058\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg entity when running a sweep.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.12.13 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/beegass/Documents/Coding/Agents/agents/dqn/wandb/run-20220408_055434-z5v1x87a</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/beegass/dqn-sweeps/runs/z5v1x87a\" target=\"_blank\">royal-sweep-4</a></strong> to <a href=\"https://wandb.ai/beegass/dqn-sweeps\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/beegass/dqn-sweeps/sweeps/150nu0fs\" target=\"_blank\">https://wandb.ai/beegass/dqn-sweeps/sweeps/150nu0fs</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/beegass/.virtualenvs/dl_1/lib/python3.8/site-packages/gym/utils/seeding.py:138: DeprecationWarning: \u001b[33mWARN: Function `hash_seed(seed, max_bytes)` is marked as deprecated and will be removed in the future. \u001b[0m\n",
      "  deprecation(\n",
      "/home/beegass/.virtualenvs/dl_1/lib/python3.8/site-packages/gym/utils/seeding.py:175: DeprecationWarning: \u001b[33mWARN: Function `_bigint_from_bytes(bytes)` is marked as deprecated and will be removed in the future. \u001b[0m\n",
      "  deprecation(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "997258e6fef84a6bac4b4e2e5f1ba8ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>episode</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>epsilon</td><td>█▅▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>mean step reward</td><td>▃▁▃▃█▁▇▂▅▅▅▁▃▁█▄▃▇▅▆▅▅▅▅▄▂▄▆▂▁▁▃▆▅▁█▁▇▃▃</td></tr><tr><td>reward per episode</td><td>▂▁▂▂▇▁▅▂▃▅▄▁▂▁▆▂▂▅█▄▄▃▄▄▂▂▄▆▂▁▁▃▄▃▁▆▁▆▃▃</td></tr><tr><td>step_count</td><td>▂▁▂▂▄▁▃▄▂▄▄▁▂▁▃▁▂▃█▂▄▂▃▄▁▄▅▄▅▅▂▄▃▂▆▃▁▄▅▄</td></tr><tr><td>total mean episodic reward</td><td>▁▁▅▇█▇▆▆▆▆▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▆▆▆▆▅▅▅▅▅▅▅▅▅▅</td></tr><tr><td>total mean episodic-step reward</td><td>▁▂▅▇█▇▇▆▅▄▄▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>total_step_count</td><td>▁▁▁▁▂▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇▇▇▇███</td></tr><tr><td>total_steps</td><td>▁▁▁▁▂▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇▇▇▇███</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>episode</td><td>9999</td></tr><tr><td>epsilon</td><td>0.0545</td></tr><tr><td>mean step reward</td><td>0.00322</td></tr><tr><td>reward per episode</td><td>1.0</td></tr><tr><td>step_count</td><td>311</td></tr><tr><td>total mean episodic reward</td><td>2.1706</td></tr><tr><td>total mean episodic-step reward</td><td>0.00768</td></tr><tr><td>total_step_count</td><td>2825232</td></tr><tr><td>total_steps</td><td>2825232</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">royal-sweep-4</strong>: <a href=\"https://wandb.ai/beegass/dqn-sweeps/runs/z5v1x87a\" target=\"_blank\">https://wandb.ai/beegass/dqn-sweeps/runs/z5v1x87a</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220408_055434-z5v1x87a/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: bg4j2yok with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdelta: 7.550379205589027\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepsilon_decay_rate: 185\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tgamma: 0.8339082430784069\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.3289044596239045\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmax_episodes: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmax_steps: 100000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tp_end: 0.05930444169965886\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \treplay_buffer_size: 18127\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttarget_freq: 331\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.1812903959445376\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg entity when running a sweep.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.12.13 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/beegass/Documents/Coding/Agents/agents/dqn/wandb/run-20220408_083358-bg4j2yok</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/beegass/dqn-sweeps/runs/bg4j2yok\" target=\"_blank\">colorful-sweep-5</a></strong> to <a href=\"https://wandb.ai/beegass/dqn-sweeps\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/beegass/dqn-sweeps/sweeps/150nu0fs\" target=\"_blank\">https://wandb.ai/beegass/dqn-sweeps/sweeps/150nu0fs</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/beegass/.virtualenvs/dl_1/lib/python3.8/site-packages/gym/utils/seeding.py:138: DeprecationWarning: \u001b[33mWARN: Function `hash_seed(seed, max_bytes)` is marked as deprecated and will be removed in the future. \u001b[0m\n",
      "  deprecation(\n",
      "/home/beegass/.virtualenvs/dl_1/lib/python3.8/site-packages/gym/utils/seeding.py:175: DeprecationWarning: \u001b[33mWARN: Function `_bigint_from_bytes(bytes)` is marked as deprecated and will be removed in the future. \u001b[0m\n",
      "  deprecation(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Ctrl + C detected. Stopping sweep.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:red\">(failed 1).</strong> Press Control-C to abort syncing."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02ea924686954bb5b3080286ab8a3890",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>episode</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>epsilon</td><td>█▅▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>mean step reward</td><td>▆▄█▄▁▃▃▅▃▇▆▄▃▆█▆▂▁▅▄▆▄▇▁▁▇▅▅▁▆▆▄▆▅▇▅▁▆▄▄</td></tr><tr><td>reward per episode</td><td>▅▃█▅▁▃▃▅▃█▆▅▃▆█▅▃▁▅▅▆▃▆▁▁▆▅▅▁▅▅▅▆▅█▅▁▅▃▃</td></tr><tr><td>step_count</td><td>▂▂▃▄▁▂▂▃▂▄▃▃▃▃▃▂█▂▂▄▄▂▃▂▁▃▃▂▂▂▂▄▃▃▄▂▁▂▂▂</td></tr><tr><td>total mean episodic reward</td><td>▁▄▇██████▇▇▇▇▇▇▇▇▇▇▇▇▇▆▆▆▆▆▆▆▆▆▆▆▆▅▅▅▆▆▆</td></tr><tr><td>total mean episodic-step reward</td><td>▃▅▇█▅▃▃▄▃▃▂▂▂▁▁▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂</td></tr><tr><td>total_step_count</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>total_steps</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>episode</td><td>8203</td></tr><tr><td>epsilon</td><td>0.0593</td></tr><tr><td>mean step reward</td><td>0.0</td></tr><tr><td>reward per episode</td><td>0.0</td></tr><tr><td>step_count</td><td>151</td></tr><tr><td>total mean episodic reward</td><td>1.95575</td></tr><tr><td>total mean episodic-step reward</td><td>0.0072</td></tr><tr><td>total_step_count</td><td>2227440</td></tr><tr><td>total_steps</td><td>2227440</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">colorful-sweep-5</strong>: <a href=\"https://wandb.ai/beegass/dqn-sweeps/runs/bg4j2yok\" target=\"_blank\">https://wandb.ai/beegass/dqn-sweeps/runs/bg4j2yok</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220408_083358-bg4j2yok/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.agent(sweep_id, run, count=10)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e235bd63d0dd74684ecb378f81599b59bf279545966289267647744a13ab7ca1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('dl_1': virtualenv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
