{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Torch Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as T\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Gym Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A.L.E: Arcade Learning Environment (version 0.7.4+069f8bd)\n",
      "[Powered by Stella]\n"
     ]
    }
   ],
   "source": [
    "from ale_py import ALEInterface\n",
    "ale = ALEInterface()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Game console created:\n",
      "  ROM file:  /home/beegass/.virtualenvs/dl_1/lib/python3.8/site-packages/AutoROM/roms/breakout.bin\n",
      "  Cart Name: Breakout - Breakaway IV (1978) (Atari)\n",
      "  Cart MD5:  f34f08e5eb96e500e851a80be3277a56\n",
      "  Display Format:  AUTO-DETECT ==> NTSC\n",
      "  ROM Size:        2048\n",
      "  Bankswitch Type: AUTO-DETECT ==> 2K\n",
      "\n",
      "Running ROM file...\n",
      "Random seed is 1647638730\n"
     ]
    }
   ],
   "source": [
    "from ale_py.roms import Breakout\n",
    "ale.loadROM(Breakout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym.wrappers import FrameStack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### All Other Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/beegass/.virtualenvs/dl_1/lib/python3.8/site-packages/matplotlib_inline/config.py:66: DeprecationWarning: InlineBackend._figure_formats_changed is deprecated in traitlets 4.1: use @observe and @unobserve instead.\n",
      "  def _figure_formats_changed(self, name, old, new):\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import trange\n",
    "import random\n",
    "import copy\n",
    "import typing\n",
    "from typing import Callable\n",
    "import PIL \n",
    "from PIL import Image\n",
    "from abc import ABC, abstractmethod\n",
    "from loguru import logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") # check if GPU is available"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The Meat And Potatoes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gym_Env():\n",
    "    def __init__(self, env_name, max_steps=1000, max_episodes=10000):\n",
    "        self.le_env = FrameStack(gym.make(env_name, render_mode='rgb_array'), 4)\n",
    "        self.max_steps = max_steps\n",
    "        self.max_episodes = max_episodes\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Replay_Buffer():\n",
    "    def __init__(self, capacity, mini_batch_size=128):\n",
    "        self.rb = []\n",
    "        self.capacity = capacity\n",
    "        self.mini_batch_size = mini_batch_size\n",
    "        self.current_batch = None\n",
    "\n",
    "    def sample_rb(self):\n",
    "        return random.sample(self.rb, batch_size=self.mini_batch_size)\n",
    "    \n",
    "    def add_to_rb(self, new_transition):\n",
    "        if len(self.rb) >= self.capacity:\n",
    "            del self.rb[0] \n",
    "        self.rb.append(new_transition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ripped from https://nn.labml.ai/rl/dqn/index.html\n",
    "class Deep_Q_Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Deep_Q_Network, self).__init__()\n",
    "        \n",
    "        self.network = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=4, out_channels=32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.lin = nn.Linear(in_features=7 * 7 * 64, out_features=512)\n",
    "        self.activation = nn.ReLU()\n",
    "        \n",
    "        self.state_value = nn.Sequential(\n",
    "            nn.Linear(in_features=512, out_features=256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=256, out_features=1)\n",
    "            )\n",
    "        \n",
    "        self.action_value = nn.Sequential(\n",
    "            nn.Linear(in_features=512, out_features=256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=256, out_features=4),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.network(x)\n",
    "        x = x.reshape((-1, 7 * 7 * 64))\n",
    "        x = self.activation(self.lin(x))\n",
    "        action_value = self.action_value(x)\n",
    "        state_value = self.state_value(x)\n",
    "        action_score_centered = action_value - action_value.mean(dim=-1, keepdim=True)\n",
    "        q = state_value + action_score_centered\n",
    "        return q  \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(nn.Module):\n",
    "    def __init__(self, pred_model):\n",
    "        super(Agent, self).__init__()\n",
    "        self.agent = pred_model\n",
    "        self.target = None\n",
    "        \n",
    "    def agent_policy(self, state, pred_model=True, grad=False):\n",
    "        q_val = -1\n",
    "        state = state.to(device)\n",
    "        if pred_model:\n",
    "            if grad:\n",
    "                q_val = self.agent(state)\n",
    "            else:\n",
    "                with torch.no_grad():\n",
    "                    q_val = self.agent(state)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                q_val = self.target(state)\n",
    "        return torch.argmax(q_val), q_val\n",
    "    \n",
    "    def copy_pred_to_target(self):\n",
    "        self.target = copy.deepcopy(self.agent)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy(environment, the_agent, state, epsilon=0.1):\n",
    "    prob = random.random()\n",
    "    action = environment.le_env.action_space.sample() # pick action from action space\n",
    "    if prob < 1 - epsilon:\n",
    "        action, _ = the_agent.agent_policy(state) # retrieve best action, based off its action-value action\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_decay(environment, episode_num, p_init=0.7, p_end=0.1):\n",
    "    episode_num += 1\n",
    "    current_episode_rate = (environment.max_episodes - episode_num) / environment.max_episodes\n",
    "    epsilon_decay_rate = max(current_episode_rate, 0)\n",
    "    return ((p_init - p_end) * (epsilon_decay_rate) + p_end)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(state):\n",
    "    \n",
    "    # convert state to numpy array and then to torch tensor\n",
    "    frame = torch.from_numpy(np.array(state).astype(np.float32))\n",
    "    \n",
    "    # reshape so that grayscaling is possible\n",
    "    reshaped_frame = frame.reshape(4, 3, 210, 160)\n",
    "    \n",
    "    # grayscale image\n",
    "    gray_frame = T.Grayscale()(reshaped_frame)\n",
    "    \n",
    "    # reshape image so network can process it\n",
    "    reshaped_gray_frame = gray_frame.reshape(1, 4, 210, 160)\n",
    "    \n",
    "    # downscale image to 84x84\n",
    "    small_gray_frame = T.Resize((84, 84))(reshaped_gray_frame)\n",
    "    \n",
    "    return small_gray_frame\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Rendering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_state(img, step=0, info=\"\"):\n",
    "    plt.figure(3)\n",
    "    plt.clf()\n",
    "    plt.imshow(img['rgb'])\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "    display.clear_output(wait=True)\n",
    "    display.display(plt.gcf())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_EPISODES = 10000\n",
    "MAX_STEPS = 10000\n",
    "REPLAY_BUFFER_SIZE = 100000\n",
    "MINI_BATCH_SIZE = 128\n",
    "EPSILON = 0.1\n",
    "GAMMA = 0.99"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Buidling Optimizers and Schedulers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_optimizer(model, optimizer_name='adam', learning_rate=0.01, weight_decay=0.01, momentum=0.9):\n",
    "    try:\n",
    "        optimizer = None\n",
    "        if optimizer_name == \"sgd\":\n",
    "            optimizer = optim.SGD(model.parameters(), \n",
    "                                  lr=learning_rate, \n",
    "                                  momentum=momentum)\n",
    "            \n",
    "        elif optimizer_name == \"adam\":\n",
    "            optimizer = optim.Adam(model.parameters(), \n",
    "                                   lr=learning_rate, \n",
    "                                   weight_decay=weight_decay)\n",
    "               \n",
    "        return optimizer\n",
    "    except:\n",
    "        print(\"Error: Invalid optimizer specified.\")\n",
    "        sys.exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_scheduler(optimizer, sched_name='reduce_lr', patience=5, verbose=True):\n",
    "    try: \n",
    "        sched = None\n",
    "        if sched_name == \"reduce_lr\":\n",
    "            sched = optim.lr_scheduler.ReduceLROnPlateau(optimizer, \n",
    "                                                         mode='min', \n",
    "                                                         patience=patience, \n",
    "                                                         verbose=verbose)\n",
    "        elif sched_name == 'TODO':\n",
    "            pass\n",
    "            #TODO: add other scheduler\n",
    "            \n",
    "        return sched\n",
    "    except:\n",
    "        logger.error(\"Error: Invalid scheduler specified.\")\n",
    "        sys.exit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(replay_buffer, the_agent, optimizer, scheduler):\n",
    "    mini_batch = replay_buffer.sample_rb()\n",
    "    \n",
    "    #1. copy weights from prediction network to target target network\n",
    "    the_agent.copy_pred_to_target()\n",
    "    \n",
    "    #2. init y_j\n",
    "    y_j = 0 \n",
    "    \n",
    "    #3. retrieve (s, a, r, s') from mini_batch\n",
    "    for transition in mini_batch:\n",
    "        (state, _, next_state, reward, done) = transition\n",
    "        _, pred_highest_q_val = the_agent.agent_policy(state, grad=True)\n",
    "        y_j = torch.FloatTensor([reward])\n",
    "        if not done:\n",
    "            _, target_highest_q_val = the_agent.agent_policy(next_state, pred_model=False)\n",
    "            y_j += GAMMA * target_highest_q_val\n",
    "        loss = nn.MSELoss(pred_highest_q_val, y_j.detach())\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step() \n",
    "    the_agent.target = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Episode Specific Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode(render, environment, the_agent, state, epsilon, optimizer, scheduler):\n",
    "    replay_buffer = Replay_Buffer(capacity=REPLAY_BUFFER_SIZE, mini_batch_size=MINI_BATCH_SIZE)\n",
    "    \n",
    "    step_count = 0\n",
    "    metadata = None\n",
    "    render_results = []\n",
    "    while True:\n",
    "        \n",
    "        #0. either explore or exploit\n",
    "        action = epsilon_greedy(environment=environment,\n",
    "                                the_agent=the_agent,\n",
    "                                state=state,\n",
    "                                epsilon=epsilon)\n",
    "        \n",
    "        #1. render environment only when flag is set to True\n",
    "        if render:\n",
    "            # TODO: render environment is currently not working\n",
    "            show_state(metadata, step=step_count)\n",
    "        \n",
    "        #2. pass action to environment\n",
    "        (next_state, reward, done, metadata) = environment.le_env.step(action)\n",
    "        \n",
    "        #3. get s' back from environment and preprocess (s' -> preprocessed_s')\n",
    "        preprocessed_next_state = preprocess(next_state)\n",
    "        \n",
    "        #4. add transition (s, a, s', r) to replay buffer\n",
    "        replay_buffer.add_to_rb((state, action, preprocessed_next_state, reward, done))\n",
    "        \n",
    "        #5. if replay buffer is full, sample mini batch and update model\n",
    "        if len(replay_buffer.rb) >= replay_buffer.capacity:\n",
    "            train(replay_buffer, the_agent, optimizer, scheduler)\n",
    "        \n",
    "        #6. check max number of time steps has been reached or if game is complete\n",
    "        if step_count >= environment.max_steps or done:\n",
    "            break\n",
    "        \n",
    "        state = preprocessed_next_state\n",
    "        \n",
    "        step_count += 1\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Episodic Loop  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(epsilon=EPSILON):\n",
    "    # initialize gym environment\n",
    "    environment = Gym_Env(env_name='ALE/Breakout-v5', max_steps=MAX_STEPS, max_episodes=MAX_EPISODES)\n",
    "    \n",
    "    # initialize prediction network\n",
    "    #pred_net = Deep_Q_Network(environment.le_env.action_space.n).to(device)\n",
    "    pred_net = Deep_Q_Network().to(device)\n",
    "    \n",
    "    # initialize agent that contains both prediction network and target network\n",
    "    the_agent = Agent(pred_model=pred_net)\n",
    "    \n",
    "    # define loss function\n",
    "    loss_fn = nn.MSELoss()\n",
    "    \n",
    "    # define optimizer\n",
    "    optimizer = build_optimizer(model=the_agent.agent, \n",
    "                                optimizer_name='adam', \n",
    "                                learning_rate=0.01, \n",
    "                                weight_decay=0.01, \n",
    "                                momentum=0.9)\n",
    "    \n",
    "    # define scheduler\n",
    "    scheduler = build_scheduler(optimizer, \n",
    "                                sched_name='reduce_lr', \n",
    "                                patience=5, \n",
    "                                verbose=True)\n",
    "    \n",
    "    render_flag = False\n",
    "    for e in range(environment.max_episodes):\n",
    "        \n",
    "        if e + 1 % 100 == 0:\n",
    "            render_flag = True\n",
    "        else:\n",
    "            render_flag = False\n",
    "        \n",
    "        # 0. get initial state, s_{0}, and preprocess it (s_{0} -> preprocessed_s)\n",
    "        state = environment.le_env.reset()\n",
    "        \n",
    "        # 0.1: preprocess state\n",
    "        preprocessed_state = preprocess(state)\n",
    "        \n",
    "        # 1. iterate over steps in episode\n",
    "        run_episode(render=render_flag,\n",
    "                    environment=environment, \n",
    "                    the_agent=the_agent,\n",
    "                    state=preprocessed_state, \n",
    "                    epsilon=epsilon, \n",
    "                    optimizer=optimizer, \n",
    "                    scheduler=scheduler)\n",
    "        \n",
    "        # 2. close rendering\n",
    "        if e + 1 % 100 == 0:\n",
    "            logger.info(f\"Episode {e+1} complete.\")\n",
    "        environment.le_env.close()\n",
    "        \n",
    "        # 3. decay epsilon\n",
    "        epsilon = epsilon_decay(environment=environment, episode_num=e)\n",
    "    logger.info(\"Training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A.L.E: Arcade Learning Environment (version 0.7.4+069f8bd)\n",
      "[Powered by Stella]\n",
      "/home/beegass/.virtualenvs/dl_1/lib/python3.8/site-packages/gym/utils/seeding.py:138: DeprecationWarning: \u001b[33mWARN: Function `hash_seed(seed, max_bytes)` is marked as deprecated and will be removed in the future. \u001b[0m\n",
      "  deprecation(\n",
      "/home/beegass/.virtualenvs/dl_1/lib/python3.8/site-packages/gym/utils/seeding.py:175: DeprecationWarning: \u001b[33mWARN: Function `_bigint_from_bytes(bytes)` is marked as deprecated and will be removed in the future. \u001b[0m\n",
      "  deprecation(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_342964/766504495.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_342964/1091053161.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(epsilon)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;31m# 1. iterate over steps in episode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         run_episode(render=render_flag,\n\u001b[0m\u001b[1;32m     44\u001b[0m                     \u001b[0menvironment\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menvironment\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m                     \u001b[0mthe_agent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mthe_agent\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_342964/2587775167.py\u001b[0m in \u001b[0;36mrun_episode\u001b[0;34m(render, environment, the_agent, state, epsilon, optimizer, scheduler)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;31m#3. get s' back from environment and preprocess (s' -> preprocessed_s')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mpreprocessed_next_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;31m#4. add transition (s, a, s', r) to replay buffer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_342964/61986213.py\u001b[0m in \u001b[0;36mpreprocess\u001b[0;34m(state)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m# grayscale image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mgray_frame\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGrayscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreshaped_frame\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m# reshape image so network can process it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/dl_1/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/dl_1/lib/python3.8/site-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m   1512\u001b[0m             \u001b[0mPIL\u001b[0m \u001b[0mImage\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mGrayscaled\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m         \"\"\"\n\u001b[0;32m-> 1514\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrgb_to_grayscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_output_channels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_output_channels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1515\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/dl_1/lib/python3.8/site-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mrgb_to_grayscale\u001b[0;34m(img, num_output_channels)\u001b[0m\n\u001b[1;32m   1169\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mF_pil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_grayscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_output_channels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1171\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mF_t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrgb_to_grayscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_output_channels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/dl_1/lib/python3.8/site-packages/torchvision/transforms/functional_tensor.py\u001b[0m in \u001b[0;36mrgb_to_grayscale\u001b[0;34m(img, num_output_channels)\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0;31m# This implementation closely follows the TF one:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0;31m# https://github.com/tensorflow/tensorflow/blob/v2.3.0/tensorflow/python/ops/image_ops_impl.py#L2105-L2138\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m     \u001b[0ml_img\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0.2989\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mr\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m0.587\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mg\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m0.114\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m     \u001b[0ml_img\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ml_img\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "run()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e235bd63d0dd74684ecb378f81599b59bf279545966289267647744a13ab7ca1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('dl_1': virtualenv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
