{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Torch Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as T\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Gym Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A.L.E: Arcade Learning Environment (version 0.7.4+069f8bd)\n",
      "[Powered by Stella]\n"
     ]
    }
   ],
   "source": [
    "from ale_py import ALEInterface\n",
    "ale = ALEInterface()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Game console created:\n",
      "  ROM file:  /home/beegass/.virtualenvs/dl_1/lib/python3.8/site-packages/AutoROM/roms/breakout.bin\n",
      "  Cart Name: Breakout - Breakaway IV (1978) (Atari)\n",
      "  Cart MD5:  f34f08e5eb96e500e851a80be3277a56\n",
      "  Display Format:  AUTO-DETECT ==> NTSC\n",
      "  ROM Size:        2048\n",
      "  Bankswitch Type: AUTO-DETECT ==> 2K\n",
      "\n",
      "Running ROM file...\n",
      "Random seed is 1647576012\n"
     ]
    }
   ],
   "source": [
    "from ale_py.roms import Breakout\n",
    "ale.loadROM(Breakout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### All Other Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/beegass/.virtualenvs/dl_1/lib/python3.8/site-packages/matplotlib_inline/config.py:66: DeprecationWarning: InlineBackend._figure_formats_changed is deprecated in traitlets 4.1: use @observe and @unobserve instead.\n",
      "  def _figure_formats_changed(self, name, old, new):\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import trange\n",
    "import random\n",
    "import copy\n",
    "import typing\n",
    "from typing import Callable\n",
    "import PIL \n",
    "from PIL import Image\n",
    "from abc import ABC, abstractmethod\n",
    "from loguru import logger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The Meat And Potatoes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gym_Env():\n",
    "    def __init__(self, env_name, max_steps=1000, max_episodes=10000):\n",
    "        self.le_env = gym.make(env_name)\n",
    "        self.max_steps = max_steps\n",
    "        self.max_episodes = max_episodes\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Replay_Buffer():\n",
    "    def __init__(self, capacity, mini_batch_size=128):\n",
    "        self.rb = []\n",
    "        self.capacity = capacity\n",
    "        self.mini_batch_size = mini_batch_size\n",
    "        self.current_batch = None\n",
    "\n",
    "    def sample_rb(self):\n",
    "        return random.sample(self.rb, batch_size=self.mini_batch_size)\n",
    "    \n",
    "    def add_to_rb(self, new_transition):\n",
    "        if len(self.rb) >= self.capacity:\n",
    "            del self.rb[0] \n",
    "        self.rb.append(new_transition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ripped from https://nn.labml.ai/rl/dqn/index.html\n",
    "class Deep_Q_Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Deep_Q_Network, self).__init__()\n",
    "        \n",
    "        self.network = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=4, out_channels=32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.lin = nn.Linear(in_features=7 * 7 * 64, out_features=512)\n",
    "        self.activation = nn.ReLU()\n",
    "        \n",
    "        self.state_value = nn.Sequential(\n",
    "            nn.Linear(in_features=512, out_features=256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=256, out_features=1)\n",
    "            )\n",
    "        \n",
    "        self.action_value = nn.Sequential(\n",
    "            nn.Linear(in_features=512, out_features=256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=256, out_features=4),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.network(x)\n",
    "        x = x.reshape((-1, 7 * 7 * 64))\n",
    "        x = self.activation(self.lin(x))\n",
    "        action_value = self.action_value(x)\n",
    "        state_value = self.state_value(x)\n",
    "        action_score_centered = action_value - action_value.mean(dim=-1, keepdim=True)\n",
    "        q = state_value + action_score_centered\n",
    "        return q  \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(nn.Module):\n",
    "    def __init__(self, pred_model):\n",
    "        super(Agent, self).__init__()\n",
    "        self.agent = pred_model\n",
    "        self.target = None\n",
    "        \n",
    "    def agent_policy(self, state, grad=False):\n",
    "        q_val = -1\n",
    "        if not grad:\n",
    "            with torch.no_grad():\n",
    "                q_val = self.agent(state)\n",
    "        else:\n",
    "            q_val = self.target(state)\n",
    "        return torch.argmax(q_val), q_val\n",
    "    \n",
    "    def copy_pred_to_target(self):\n",
    "        self.target = copy.deepcopy(self.agent)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy(environment, the_agent, state, epsilon=0.1):\n",
    "    prob = random.random()\n",
    "    action = environment.le_env.action_space.sample() # pick action from action space\n",
    "    if prob < 1 - epsilon:\n",
    "        action, _ = the_agent.agent_policy(state) # retrieve best action, based off its action-value action\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_decay(environment, episode_num, p_init=0.7, p_end=0.1):\n",
    "    episode_num += 1\n",
    "    current_episode_rate = (environment.max_episodes - episode_num) / environment.max_episodes\n",
    "    epsilon_decay_rate = max(current_episode_rate, 0)\n",
    "    return ((p_init - p_end) * (epsilon_decay_rate) + p_end)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(state):\n",
    "    logger.info(f\"----Before Processing----\")\n",
    "    logger.info(f\"state shape: {state.shape}\")\n",
    "    logger.info(f\"state type: {type(state)}\")\n",
    "    frame = torch.from_numpy(state)\n",
    "    reshaped_frame = frame.reshape(3, 210, 160)\n",
    "    gray_frame = T.Grayscale()(reshaped_frame)\n",
    "    small_gray_frame = T.Resize((84, 84))(gray_frame)\n",
    "    logger.info(f\"----After Processing----\")\n",
    "    logger.info(f\"state shape: {small_gray_frame.shape}\")\n",
    "    logger.info(f\"state type: {type(small_gray_frame)}\")\n",
    "    return small_gray_frame\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") # check if GPU is available\n",
    "MAX_EPISODES = 10000\n",
    "MAX_STEPS = 10000\n",
    "REPLAY_BUFFER_SIZE = 5000\n",
    "MINI_BATCH_SIZE = 36\n",
    "EPSILON = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Buidling Optimizers and Schedulers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_optimizer(model, optimizer_name='adam', learning_rate=0.01, weight_decay=0.01, momentum=0.9):\n",
    "    try:\n",
    "        optimizer = None\n",
    "        if optimizer_name == \"sgd\":\n",
    "            optimizer = optim.SGD(model.parameters(), \n",
    "                                  lr=learning_rate, \n",
    "                                  momentum=momentum)\n",
    "            \n",
    "        elif optimizer_name == \"adam\":\n",
    "            optimizer = optim.Adam(model.parameters(), \n",
    "                                   lr=learning_rate, \n",
    "                                   weight_decay=weight_decay)\n",
    "               \n",
    "        return optimizer\n",
    "    except:\n",
    "        print(\"Error: Invalid optimizer specified.\")\n",
    "        sys.exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_scheduler(optimizer, sched_name='reduce_lr', patience=5, verbose=True):\n",
    "    try: \n",
    "        sched = None\n",
    "        if sched_name == \"reduce_lr\":\n",
    "            sched = optim.lr_scheduler.ReduceLROnPlateau(optimizer, \n",
    "                                                         mode='min', \n",
    "                                                         patience=patience, \n",
    "                                                         verbose=verbose)\n",
    "        elif sched_name == 'TODO':\n",
    "            pass\n",
    "            #TODO: add other scheduler\n",
    "            \n",
    "        return sched\n",
    "    except:\n",
    "        logger.error(\"Error: Invalid scheduler specified.\")\n",
    "        sys.exit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(replay_buffer, the_agent, optimizer, scheduler):\n",
    "    mini_batch = replay_buffer.sample_rb()\n",
    "    \n",
    "    #1. copy weights from pred to target\n",
    "    the_agent.copy_pred_to_target()\n",
    "    \n",
    "    #2. init y_j\n",
    "    y_j = 0 \n",
    "    \n",
    "    #3. retrieve (s, a, r, s') from mini_batch\n",
    "    for transition in mini_batch:\n",
    "        (state, _, next_state, reward, done) = transition\n",
    "        state = state.to(device)\n",
    "        _, pred_highest_q_val = the_agent.agent_policy(state, grad=True)\n",
    "        y_j = torch.FloatTensor([reward])\n",
    "        if not done:\n",
    "            _, target_highest_q_val = the_agent.agent_policy(next_state, grad=False)\n",
    "            y_j += GAMMA * target_highest_q_val\n",
    "        loss = nn.MSELoss(pred_highest_q_val, y_j.detach())\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step() \n",
    "    the_agent.target = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode(environment, the_agent, state, epsilon, optimizer, scheduler):\n",
    "    replay_buffer = Replay_Buffer(capacity=REPLAY_BUFFER_SIZE, mini_batch_size=MINI_BATCH_SIZE)\n",
    "    \n",
    "    step_count = 0\n",
    "    while True:\n",
    "        \n",
    "        logger.info(f\"state before NN: {type(state)}\")\n",
    "        logger.info(f\"state before NN: {state.shape}\")\n",
    "        \n",
    "        #0. either explore or exploit\n",
    "        action = epsilon_greedy(environment=environment,\n",
    "                                the_agent=the_agent,\n",
    "                                state=state,\n",
    "                                epsilon=epsilon)\n",
    "        \n",
    "        #1. pass action to environment\n",
    "        (next_state, reward, done, _) = environment.le_env.step(action)\n",
    "        \n",
    "        #2. TODO: get s' back from environment and preprocess (s' -> preprocessed_s')\n",
    "        preprocessed_next_state = preprocess(next_state)\n",
    "        \n",
    "        #3. add transition (s, a, s', r) to replay buffer\n",
    "        replay_buffer.add_to_rb((state, action, preprocessed_next_state, reward, done))\n",
    "        state = preprocessed_next_state\n",
    "        \n",
    "        #4. if replay buffer is full, sample mini batch and update model\n",
    "        if len(replay_buffer.rb) >= replay_buffer.capacity:\n",
    "            train(replay_buffer, the_agent, optimizer, scheduler)\n",
    "        \n",
    "        #5. check max number of time steps has been reached or if game is complete\n",
    "        if step_count >= environment.max_steps or done:\n",
    "            break\n",
    "        \n",
    "        state = next_state\n",
    "        \n",
    "        step_count += 1\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(epsilon=EPSILON):\n",
    "    # initialize gym environment\n",
    "    environment = Gym_Env(env_name='ALE/Breakout-v5', max_steps=MAX_STEPS, max_episodes=MAX_EPISODES)\n",
    "    \n",
    "    # initialize prediction network\n",
    "    pred_net = Deep_Q_Network(environment.le_env.action_space.n).to(device)\n",
    "    \n",
    "    # initialize agent that contains both prediction network and target network\n",
    "    the_agent = Agent(pred_model=pred_net)\n",
    "    \n",
    "    # define loss function\n",
    "    loss_fn = nn.MSELoss()\n",
    "    \n",
    "    # define optimizer\n",
    "    optimizer = build_optimizer(model=the_agent.agent, \n",
    "                                optimizer_name='adam', \n",
    "                                learning_rate=0.01, \n",
    "                                weight_decay=0.01, \n",
    "                                momentum=0.9)\n",
    "    \n",
    "    # define scheduler\n",
    "    scheduler = build_scheduler(optimizer, \n",
    "                                sched_name='reduce_lr', \n",
    "                                patience=5, \n",
    "                                verbose=True)\n",
    "    \n",
    "    global EPSILON\n",
    "    epsilon = EPSILON\n",
    "    for e in range(environment.max_episodes):\n",
    "        \n",
    "        # 0. get initial state, s_{0}, and preprocess it (s_{0} -> preprocessed_s)\n",
    "        state = environment.le_env.reset()\n",
    "        \n",
    "        # 0.1: TODO: preprocess state\n",
    "        preprocessed_state = preprocess(state)\n",
    "        \n",
    "        # 1. iterate over steps in episode\n",
    "        run_episode(environment=environment, \n",
    "                    the_agent=the_agent,\n",
    "                    state=preprocessed_state, \n",
    "                    epsilon=epsilon, \n",
    "                    optimizer=optimizer, \n",
    "                    scheduler=scheduler)\n",
    "        \n",
    "        # 2. decay epsilon\n",
    "        epsilon = epsilon_decay(environment=environment, episode_num=e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A.L.E: Arcade Learning Environment (version 0.7.4+069f8bd)\n",
      "[Powered by Stella]\n",
      "/home/beegass/.virtualenvs/dl_1/lib/python3.8/site-packages/gym/utils/seeding.py:138: DeprecationWarning: \u001b[33mWARN: Function `hash_seed(seed, max_bytes)` is marked as deprecated and will be removed in the future. \u001b[0m\n",
      "  deprecation(\n",
      "/home/beegass/.virtualenvs/dl_1/lib/python3.8/site-packages/gym/utils/seeding.py:175: DeprecationWarning: \u001b[33mWARN: Function `_bigint_from_bytes(bytes)` is marked as deprecated and will be removed in the future. \u001b[0m\n",
      "  deprecation(\n",
      "2022-03-18 00:00:16.793 | INFO     | __main__:preprocess:2 - ----Before Processing----\n",
      "2022-03-18 00:00:16.794 | INFO     | __main__:preprocess:3 - state shape: (210, 160, 3)\n",
      "2022-03-18 00:00:16.795 | INFO     | __main__:preprocess:4 - state type: <class 'numpy.ndarray'>\n",
      "2022-03-18 00:00:16.796 | INFO     | __main__:preprocess:9 - ----After Processing----\n",
      "2022-03-18 00:00:16.797 | INFO     | __main__:preprocess:10 - state shape: torch.Size([1, 84, 84])\n",
      "2022-03-18 00:00:16.798 | INFO     | __main__:preprocess:11 - state type: <class 'torch.Tensor'>\n",
      "2022-03-18 00:00:16.798 | INFO     | __main__:run_episode:7 - state before NN: <class 'torch.Tensor'>\n",
      "2022-03-18 00:00:16.799 | INFO     | __main__:run_episode:8 - state before NN: torch.Size([1, 84, 84])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected 4-dimensional input for 4-dimensional weight [32, 4, 8, 8], but got 3-dimensional input of size [1, 84, 84] instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_492312/766504495.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_492312/1529585433.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(epsilon)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;31m# 1. iterate over steps in episode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         run_episode(environment=environment, \n\u001b[0m\u001b[1;32m     39\u001b[0m                     \u001b[0mthe_agent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mthe_agent\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m                     \u001b[0mstate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpreprocessed_state\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_492312/2030278521.py\u001b[0m in \u001b[0;36mrun_episode\u001b[0;34m(environment, the_agent, state, epsilon, optimizer, scheduler)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;31m#0. either explore or exploit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         action = epsilon_greedy(environment=environment,\n\u001b[0m\u001b[1;32m     12\u001b[0m                                 \u001b[0mthe_agent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mthe_agent\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m                                 \u001b[0mstate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_492312/3746690154.py\u001b[0m in \u001b[0;36mepsilon_greedy\u001b[0;34m(environment, the_agent, state, epsilon)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menvironment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mle_env\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# pick action from action space\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mprob\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mthe_agent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magent_policy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# retrieve best action, based off its action-value action\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_492312/1396773390.py\u001b[0m in \u001b[0;36magent_policy\u001b[0;34m(self, state, grad)\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m                 \u001b[0mq_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0mq_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/dl_1/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_492312/2211433990.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/dl_1/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/dl_1/lib/python3.8/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/dl_1/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/dl_1/lib/python3.8/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 446\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/dl_1/lib/python3.8/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    440\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--> 442\u001b[0;31m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[1;32m    443\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected 4-dimensional input for 4-dimensional weight [32, 4, 8, 8], but got 3-dimensional input of size [1, 84, 84] instead"
     ]
    }
   ],
   "source": [
    "run()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e235bd63d0dd74684ecb378f81599b59bf279545966289267647744a13ab7ca1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('dl_1': virtualenv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
