{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Torch Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Gym Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym.wrappers import FrameStack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### All Other Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import trange\n",
    "import random\n",
    "import copy\n",
    "import typing\n",
    "from typing import Callable \n",
    "from abc import ABC, abstractmethod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gym_Env():\n",
    "    def __init__(self, env_name, max_steps=1000, max_episodes=10000):\n",
    "        self.env = gym.make(env_name)\n",
    "        self.max_steps = max_steps\n",
    "        self.max_episodes = max_episodes\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transition():\n",
    "    def __init__(self, state, action, next_state, reward, done):\n",
    "        self.state = state\n",
    "        self.action = action\n",
    "        self.reward = reward\n",
    "        self.next_state = next_state\n",
    "        self.game_complete = done\n",
    "        self.transition = (self.state, self.action, self.next_state, self.reward)\n",
    "    \n",
    "    def change_state(self, state):\n",
    "        return self.next_state = state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Replay_Buffer():\n",
    "    def __init__(self, capacity, mini_batch_size):\n",
    "        self.rb = []\n",
    "        self.capacity = capacity\n",
    "        self.mini_batch_size = mini_batch_size\n",
    "        self.current_batch = None\n",
    "    \n",
    "    def sample_rb(self):\n",
    "        self.current_batch = random.sample(self.rb, batch_size=self.mini_batch_size)\n",
    "    \n",
    "    def add_to_rb(self, new_transition):\n",
    "        if len(self.rb) >= self.capacity:\n",
    "            del self.rb[0] \n",
    "        self.rb.append(new_transition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Deep_Q_Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Deep_Q_Network_Agent, self).__init__()\n",
    "        \n",
    "        self.network = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, kernel_size=5, stride=2),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 32, kernel_size=5, stride=2),\n",
    "            n.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 32, kernel_size=5, stride=2),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(nn.Module):\n",
    "    def __init__(self, pred_model):\n",
    "        self.agent = pred_model\n",
    "        self.target = self.copy_pred_to_target()\n",
    "        \n",
    "    def get_action_val(self, state):\n",
    "        with torch.no_grad():\n",
    "            q_val = self.agent(state)\n",
    "        return torch.argmax(q_val)\n",
    "    \n",
    "    def copy_pred_to_target(self):\n",
    "        self.target = copy.deepcopy(self.agent)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data_Preprocess(ABC):\n",
    "    \n",
    "    @abstractmethod\n",
    "    def preprocess_state(self):\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def preprocess_state(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") # check if GPU is available\n",
    "MAX_EPISODES = None\n",
    "MAX_STEPS = None\n",
    "REPLAY_BUFFER_SIZE = None\n",
    "MINI_BATCH_SIZE = None\n",
    "EPSILON = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Buidling Optimizers and Schedulers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_optimizer(model, optimizer_name='adam', learning_rate=0.01, weight_decay=0.01, momentum=0.9):\n",
    "    try:\n",
    "        optimizer = None\n",
    "        if optimizer_name == \"sgd\":\n",
    "            optimizer = optim.SGD(model.parameters(), \n",
    "                                  lr=learning_rate, \n",
    "                                  momentum=momentum)\n",
    "            \n",
    "        elif optimizer_name == \"adam\":\n",
    "            optimizer = optim.Adam(model.parameters(), \n",
    "                                   lr=learning_rate, \n",
    "                                   weight_decay=weight_decay)\n",
    "               \n",
    "        return optimizer\n",
    "    except:\n",
    "        print(\"Error: Invalid optimizer specified.\")\n",
    "        sys.exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_scheduler(optimizer, sched_name='reduce_lr', patience=5, verbose=True):\n",
    "    try: \n",
    "        sched = None\n",
    "        if sched_name == \"reduce_lr\":\n",
    "            sched = optim.lr_scheduler.ReduceLROnPlateau(optimizer, \n",
    "                                                         mode='min', \n",
    "                                                         patience=patience, \n",
    "                                                         verbose=verbose)\n",
    "        elif sched_name == 'TODO':\n",
    "            pass\n",
    "            #TODO: add other scheduler\n",
    "            \n",
    "        return sched\n",
    "    except:\n",
    "        print(\"Error: Invalid scheduler specified.\")\n",
    "        sys.exit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(replay_buffer):\n",
    "    replay_buffer.sample_rb()\n",
    "    mini_batch = replay_buffer.current_batch\\\n",
    "    \n",
    "    #1. copy weights from pred to target\n",
    "    \n",
    "    #2. retrieve (s, a, r, s') from mini_batch\n",
    "    \n",
    "    #3.1 pass to pred model s from (s, a, r, s')\n",
    "    \n",
    "    #3.2 pass to target model s' from (s, a, r, s')\n",
    "    #hint: make sure target model is not performing back propagation\n",
    "    \n",
    "    #4. pass values generated from pred model to target model to bellman equation\n",
    "    \n",
    "    #5. use loss to perform back propagation on pred model\n",
    "    #hint: use bellman equation to evaluate loss \n",
    "    \n",
    "    #6. destory target model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode(environment, the_agent, loss_fn, optimizer, scheduler):\n",
    "    replay_buffer = Replay_Buffer(capacity=REPLAY_BUFFER_SIZE, mini_batch_size=MINI_BATCH_SIZE)\n",
    "    pred_agent = the_agent.agent\n",
    "    #0. get initial state, s_{0}, and preprocess it (s_{0} -> preprocessed_s)\n",
    "    # TODO: preprocess s_{0}\n",
    "    \n",
    "    episode_active = True\n",
    "    step_count = 0\n",
    "    while episode_active and (step_count < environment.max_steps):\n",
    "        random_action_prob = random.uniform(0.0, 1.0)\n",
    "        if random_action_prob < EPSILON:\n",
    "            action = environment.env.action_space.sample()\n",
    "        else:\n",
    "            action = pred_agent.get_action_val(state)\n",
    "        \n",
    "        #1. pass action to environment\n",
    "        next_state, reward, done, _ = environment.env.step(action)\n",
    "        \n",
    "        #2. get s' back from enviroment and preprocess (s' -> preprocessed_s')\n",
    "        # TODO: preprocess s'\n",
    "        \n",
    "        #3. add transition (s, a, s', r) to replay buffer\n",
    "        replay_buffer.add_to_rb(Transition(state=state, action=action, next_state=next_state, reward=reward, done=done))\n",
    "        \n",
    "        #4. if replay buffer is full, sample mini batch and update model\n",
    "        if len(replay_buffer.rb) == replay_buffer.capacity:\n",
    "            train(replay_buffer, the_agent, loss_fn, optimizer, scheduler)\n",
    "        \n",
    "        #5. check max number of time steps has been reached or if game is complete\n",
    "        if step_count >= environment.max_steps or done:\n",
    "            episode_active = False\n",
    "            environment.env.reset()\n",
    "            \n",
    "        step_count += 1\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run():\n",
    "    # initialize variables for epsilon decay\n",
    "    p_init = 0.7\n",
    "    p_end = 0.1\n",
    "    \n",
    "    # initialize gym environment\n",
    "    environment = Gym_Env(env_name='CartPole-v1', max_steps=MAX_STEPS, max_episodes=MAX_EPISODES)\n",
    "    \n",
    "    # initialize prediction network\n",
    "    pred_net = Deep_Q_Network()\n",
    "    \n",
    "    # initialize agent that contains both prediction network and target network\n",
    "    the_agent = Agent(pred_model=pred_model)\n",
    "    \n",
    "    # define loss function\n",
    "    loss_fn = nn.MSELoss()\n",
    "    \n",
    "    # define optimizer\n",
    "    optimizer = build_optimizer(model=the_agent.agent, \n",
    "                                optimizer_name='adam', \n",
    "                                learning_rate=0.01, \n",
    "                                weight_decay=0.01, \n",
    "                                momentum=0.9)\n",
    "    \n",
    "    # define scheduler\n",
    "    scheduler = build_scheduler(optimizer, \n",
    "                                sched_name='reduce_lr', \n",
    "                                patience=5, \n",
    "                                verbose=True)\n",
    "    \n",
    "    episode = 0 # episode counter\n",
    "    for e in trange(environment.max_episodes):\n",
    "        run_episode(environment, the_agent, loss_fn, optimizer, scheduler)\n",
    "        episode = e + 1\n",
    "        current_episode_rate = (environment.max_episodes - episode) / environment.max_episodes\n",
    "        epsilon_decay_rate = max(current_episode_rate, 0)\n",
    "        epsilon = ((p_init - p_end) * epsilon_decay_rate) + p_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e235bd63d0dd74684ecb378f81599b59bf279545966289267647744a13ab7ca1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('dl_1': virtualenv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
