{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Torch Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as T\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Gym Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym.wrappers import FrameStack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### All Other Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import trange\n",
    "import random\n",
    "import copy\n",
    "import typing\n",
    "from typing import Callable\n",
    "import PIL \n",
    "from PIL import Image\n",
    "from abc import ABC, abstractmethod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The Meat And Potatoes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gym_Env():\n",
    "    def __init__(self, env_name, max_steps=1000, max_episodes=10000):\n",
    "        self.env = gym.make(env_name)\n",
    "        self.max_steps = max_steps\n",
    "        self.max_episodes = max_episodes\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Replay_Buffer():\n",
    "    def __init__(self, capacity, mini_batch_size):\n",
    "        self.rb = []\n",
    "        self.capacity = capacity\n",
    "        self.mini_batch_size = mini_batch_size\n",
    "        self.current_batch = None\n",
    "\n",
    "    def sample_rb(self):\n",
    "        self.current_batch = random.sample(self.rb, batch_size=self.mini_batch_size)\n",
    "    \n",
    "    def add_to_rb(self, new_transition):\n",
    "        if len(self.rb) >= self.capacity:\n",
    "            del self.rb[0] \n",
    "        self.rb.append(new_transition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Deep_Q_Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Deep_Q_Network, self).__init__()\n",
    "        \n",
    "        self.network = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, kernel_size=5, stride=2),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 32, kernel_size=5, stride=2),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 32, kernel_size=5, stride=2),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(nn.Module):\n",
    "    def __init__(self, pred_model):\n",
    "        super(Agent, self).__init__()\n",
    "        self.agent = pred_model\n",
    "        self.target = None\n",
    "        \n",
    "    def get_action_val(self, state, no_grad=True):\n",
    "        if no_grad:\n",
    "            with torch.no_grad():\n",
    "                q_val = self.agent(state)\n",
    "        else:\n",
    "            q_val = self.agent(state)\n",
    "        return torch.argmax(q_val), q_val\n",
    "    \n",
    "    def copy_pred_to_target(self):\n",
    "        self.target = copy.deepcopy(self.agent)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "resize = T.Compose([T.ToPILImage(),\n",
    "                            T.Resize(40, interpolation = Image.CUBIC),\n",
    "                            T.ToTensor()])\n",
    "\n",
    "def get_cart_location(screen_width, state, environment):\n",
    "    world_width = environment.env.x_threshold * 2\n",
    "    scale = screen_width / world_width\n",
    "    # return middle/center location of the cart body\n",
    "    return int(state * scale + screen_width / 2.0)\n",
    "\n",
    "def get_screen(state, environment):\n",
    "    screen = environment.env.render(mode='rgb_array').transpose((2, 0, 1))\n",
    "    # crop image as cart is in lower half, top and bottom screen unimportant\n",
    "    _, screen_height, screen_width = screen.shape\n",
    "    screen = screen[:, int(screen_height*0.4):int(screen_height * 0.8)]\n",
    "    view_width = int(screen_width * 0.6)\n",
    "    cart_location = get_cart_location(screen_width, state, environment)\n",
    "    if cart_location < view_width // 2:\n",
    "        slice_range = slice(view_width)\n",
    "    elif cart_location > (screen_width - view_width // 2):\n",
    "        slice_range = slice(-view_width, None)\n",
    "    else:\n",
    "        slice_range = slice(cart_location - view_width // 2,\n",
    "                            cart_location + view_width // 2)\n",
    "    # crop so that we have square image centered on cart\n",
    "    screen = screen[:, :, slice_range]\n",
    "    # convert to float, rescale and conver \n",
    "    screen = np.ascontiguousarray(screen, dtype=np.float32) / 255\n",
    "    # convert to tensor\n",
    "    screen = torch.from_numpy(screen)\n",
    "    # screen capture is of size 3x160x360\n",
    "    # insert singleton dim at index 0 \n",
    "    # to change size to 1x3x40x90\n",
    "    # size follows from (160) or (360) / (1 img + 3 channels)\n",
    "    return resize(screen).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") # check if GPU is available\n",
    "MAX_EPISODES = 10000\n",
    "MAX_STEPS = 10000\n",
    "REPLAY_BUFFER_SIZE = 5000\n",
    "MINI_BATCH_SIZE = 36\n",
    "EPSILON = 0.95"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Buidling Optimizers and Schedulers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_optimizer(model, optimizer_name='adam', learning_rate=0.01, weight_decay=0.01, momentum=0.9):\n",
    "    try:\n",
    "        optimizer = None\n",
    "        if optimizer_name == \"sgd\":\n",
    "            optimizer = optim.SGD(model.parameters(), \n",
    "                                  lr=learning_rate, \n",
    "                                  momentum=momentum)\n",
    "            \n",
    "        elif optimizer_name == \"adam\":\n",
    "            optimizer = optim.Adam(model.parameters(), \n",
    "                                   lr=learning_rate, \n",
    "                                   weight_decay=weight_decay)\n",
    "               \n",
    "        return optimizer\n",
    "    except:\n",
    "        print(\"Error: Invalid optimizer specified.\")\n",
    "        sys.exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_scheduler(optimizer, sched_name='reduce_lr', patience=5, verbose=True):\n",
    "    try: \n",
    "        sched = None\n",
    "        if sched_name == \"reduce_lr\":\n",
    "            sched = optim.lr_scheduler.ReduceLROnPlateau(optimizer, \n",
    "                                                         mode='min', \n",
    "                                                         patience=patience, \n",
    "                                                         verbose=verbose)\n",
    "        elif sched_name == 'TODO':\n",
    "            pass\n",
    "            #TODO: add other scheduler\n",
    "            \n",
    "        return sched\n",
    "    except:\n",
    "        print(\"Error: Invalid scheduler specified.\")\n",
    "        sys.exit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_q_val(state, is_pred=True, no_grad=False):\n",
    "    if is_pred:\n",
    "        action_val, q_val_arr  = the_agent.agent.get_action_val(state, no_grad=no_grad)\n",
    "    else: \n",
    "        action_val, q_val_arr  = the_agent.target.get_action_val(state)\n",
    "    return pred_q_val_arr[pred_action_val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(replay_buffer, the_agent, loss_fn, optimizer, scheduler):\n",
    "    replay_buffer.sample_rb()\n",
    "    mini_batch = replay_buffer.current_batch\\\n",
    "    \n",
    "    #1. copy weights from pred to target\n",
    "    the_agent.copy_pred_to_target()\n",
    "    \n",
    "    #2. retrieve (s, a, r, s') from mini_batch\n",
    "    for transition in mini_batch:\n",
    "        (state, action, next_state, reward, done) = transition\n",
    "        pred_highest_q_val = get_best_q_val(state)\n",
    "        y_j = torch.FloatTensor([reward])\n",
    "        if not done:\n",
    "            target_highest_q_val = get_best_q_val(state, is_pred=False, no_grad=True)\n",
    "            y_j += GAMMA * target_highest_q_val\n",
    "        loss = loss_fn(pred_highest_q_val, y_j.detach())\n",
    "        loss.backward()\n",
    "        optimizer.step() \n",
    "    the_agent.target = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode(environment, the_agent, loss_fn, optimizer, scheduler):\n",
    "    replay_buffer = Replay_Buffer(capacity=REPLAY_BUFFER_SIZE, mini_batch_size=MINI_BATCH_SIZE)\n",
    "    pred_agent = the_agent.agent\n",
    "    #0. get initial state, s_{0}, and preprocess it (s_{0} -> preprocessed_s)\n",
    "    state = environment.env.reset()\n",
    "    state = get_screen(state, environment)\n",
    "    action = 0\n",
    "    next_state = state\n",
    "    reward = 0\n",
    "    done = False\n",
    "    \n",
    "    episode_active = True\n",
    "    step_count = 0\n",
    "    while episode_active and (step_count < environment.max_steps):\n",
    "        random_action_prob = random.uniform(0.0, 1.0)\n",
    "        if random_action_prob < EPSILON:\n",
    "            action = environment.env.action_space.sample()\n",
    "        else:\n",
    "            action, _ = pred_agent.get_action_val(state)\n",
    "        \n",
    "        #1. pass action to environment\n",
    "        next_state, reward, done, _ = environment.env.step(action)\n",
    "        \n",
    "        #2. get s' back from environment and preprocess (s' -> preprocessed_s')\n",
    "        next_state = get_screen(next_state, environment)\n",
    "        \n",
    "        #3. add transition (s, a, s', r) to replay buffer\n",
    "        replay_buffer.add_to_rb((state, action, next_state, reward, done))\n",
    "        state = next_state\n",
    "        \n",
    "        #4. if replay buffer is full, sample mini batch and update model\n",
    "        if len(replay_buffer.rb) == replay_buffer.capacity:\n",
    "            train(replay_buffer, the_agent, loss_fn, optimizer, scheduler)\n",
    "        \n",
    "        #5. check max number of time steps has been reached or if game is complete\n",
    "        if step_count >= environment.max_steps or done:\n",
    "            episode_active = False\n",
    "            environment.env.reset()\n",
    "            \n",
    "        step_count += 1\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run():\n",
    "    # initialize variables for epsilon decay\n",
    "    p_init = 0.7\n",
    "    p_end = 0.1\n",
    "    \n",
    "    # initialize gym environment\n",
    "    environment = Gym_Env(env_name='CartPole-v1', max_steps=MAX_STEPS, max_episodes=MAX_EPISODES)\n",
    "    \n",
    "    # initialize prediction network\n",
    "    pred_net = Deep_Q_Network()\n",
    "    \n",
    "    # initialize agent that contains both prediction network and target network\n",
    "    the_agent = Agent(pred_model=pred_net)\n",
    "    \n",
    "    # define loss function\n",
    "    loss_fn = nn.MSELoss()\n",
    "    \n",
    "    # define optimizer\n",
    "    optimizer = build_optimizer(model=the_agent.agent, \n",
    "                                optimizer_name='adam', \n",
    "                                learning_rate=0.01, \n",
    "                                weight_decay=0.01, \n",
    "                                momentum=0.9)\n",
    "    \n",
    "    # define scheduler\n",
    "    scheduler = build_scheduler(optimizer, \n",
    "                                sched_name='reduce_lr', \n",
    "                                patience=5, \n",
    "                                verbose=True)\n",
    "    \n",
    "    episode = 0 # episode counter\n",
    "    for e in trange(environment.max_episodes):\n",
    "        run_episode(environment, the_agent, loss_fn, optimizer, scheduler)\n",
    "        episode = e + 1\n",
    "        current_episode_rate = (environment.max_episodes - episode) / environment.max_episodes\n",
    "        epsilon_decay_rate = max(current_episode_rate, 0)\n",
    "        EPSILON = ((p_init - p_end) * epsilon_decay_rate) + p_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10000 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "only size-1 arrays can be converted to Python scalars",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1255861/766504495.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_1255861/4289356178.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mepisode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;31m# episode counter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menvironment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_episodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0mrun_episode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menvironment\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthe_agent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0mepisode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mcurrent_episode_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0menvironment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_episodes\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mepisode\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0menvironment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_episodes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_1255861/711373975.py\u001b[0m in \u001b[0;36mrun_episode\u001b[0;34m(environment, the_agent, loss_fn, optimizer, scheduler)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m#0. get initial state, s_{0}, and preprocess it (s_{0} -> preprocessed_s)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menvironment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_screen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menvironment\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mnext_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_1255861/2876851354.py\u001b[0m in \u001b[0;36mget_screen\u001b[0;34m(state, environment)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mscreen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscreen\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscreen_height\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m0.4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscreen_height\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m0.8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mview_width\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscreen_width\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m0.6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mcart_location\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_cart_location\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscreen_width\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menvironment\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcart_location\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mview_width\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mslice_range\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mview_width\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_1255861/2876851354.py\u001b[0m in \u001b[0;36mget_cart_location\u001b[0;34m(screen_width, state, environment)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mscale\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscreen_width\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mworld_width\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m# return middle/center location of the cart body\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mscale\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mscreen_width\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m2.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_screen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menvironment\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: only size-1 arrays can be converted to Python scalars"
     ]
    }
   ],
   "source": [
    "run()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e235bd63d0dd74684ecb378f81599b59bf279545966289267647744a13ab7ca1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('dl_1': virtualenv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
