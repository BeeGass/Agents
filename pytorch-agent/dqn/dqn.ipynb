{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Torch Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as T\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Gym Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A.L.E: Arcade Learning Environment (version 0.7.4+069f8bd)\n",
      "[Powered by Stella]\n"
     ]
    }
   ],
   "source": [
    "from ale_py import ALEInterface\n",
    "ale = ALEInterface()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Game console created:\n",
      "  ROM file:  /home/beegass/.virtualenvs/dl_1/lib/python3.8/site-packages/AutoROM/roms/breakout.bin\n",
      "  Cart Name: Breakout - Breakaway IV (1978) (Atari)\n",
      "  Cart MD5:  f34f08e5eb96e500e851a80be3277a56\n",
      "  Display Format:  AUTO-DETECT ==> NTSC\n",
      "  ROM Size:        2048\n",
      "  Bankswitch Type: AUTO-DETECT ==> 2K\n",
      "\n",
      "Running ROM file...\n",
      "Random seed is 1652076930\n"
     ]
    }
   ],
   "source": [
    "from ale_py.roms import Breakout\n",
    "ale.loadROM(Breakout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym.wrappers import (\n",
    "    FrameStack, \n",
    "    AtariPreprocessing, \n",
    "    RecordEpisodeStatistics\n",
    ")\n",
    "\n",
    "from stable_baselines3.common.atari_wrappers import (\n",
    "    ClipRewardEnv,\n",
    "    EpisodicLifeEnv,\n",
    "    FireResetEnv,\n",
    "    MaxAndSkipEnv,\n",
    "    NoopResetEnv\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### All Other Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import trange\n",
    "import random\n",
    "import copy\n",
    "from loguru import logger\n",
    "import wandb\n",
    "import time\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbeegass\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/beegass/.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login(key=\"5966d774f384473f3d7ed674ef762b1a26a54d63\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") # check if GPU is available"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The Meat And Potatoes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env(env_name, seed=42):\n",
    "    env = gym.make(env_name, frameskip=1, repeat_action_probability=0)\n",
    "    env = AtariPreprocessing(env)\n",
    "    env = RecordEpisodeStatistics(env)\n",
    "    env = ClipRewardEnv(env)\n",
    "    env = EpisodicLifeEnv(env)\n",
    "    env = FrameStack(env, 4)\n",
    "    env.seed(seed)\n",
    "    env.action_space.seed(seed)\n",
    "    env.observation_space.seed(seed)\n",
    "    return env \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Replay_Buffer():\n",
    "    def __init__(self, capacity, mini_batch_size=128):\n",
    "        self.rb = []\n",
    "        self.capacity = capacity\n",
    "        self.mini_batch_size = mini_batch_size\n",
    "\n",
    "    def get_rb_batch(self):\n",
    "        sample = random.sample(self.rb, self.mini_batch_size)\n",
    "        states, actions, next_states, rewards, done = zip(*sample[:(self.mini_batch_size)])\n",
    "        preprocessed_states = preprocess_two(states)\n",
    "        preprocessed_next_states = preprocess_two(next_states)\n",
    "        return preprocessed_states, actions, preprocessed_next_states, rewards, done\n",
    "    \n",
    "    def add_to_rb(self, new_transition):\n",
    "        if len(self.rb) >= self.capacity:\n",
    "            del self.rb[0] \n",
    "        self.rb.append(new_transition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, in_channels=4, num_actions=4):\n",
    "        super(DQN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, 32, kernel_size=8, stride=4)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
    "        self.fc4 = nn.Linear(7 * 7 * 64, 512)\n",
    "        self.fc5 = nn.Linear(512, num_actions)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.fc4(x.view(x.size(0), -1)))\n",
    "        return self.fc5(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class DQN(nn.Module):\n",
    "#     def __init__(self, in_channels=4, num_actions=4, img_h=84, img_w=84):\n",
    "#         super(DQN, self).__init__()\n",
    "#         self.conv1 = nn.Conv2d(in_channels, 16, kernel_size=5, stride=2)\n",
    "#         self.bn1 = nn.BatchNorm2d(16)\n",
    "#         self.conv2 = nn.Conv2d(16, 32, kernel_size=5, stride=2)\n",
    "#         self.bn2 = nn.BatchNorm2d(32)\n",
    "#         self.conv3 = nn.Conv2d(32, 32, kernel_size=5, stride=2)\n",
    "#         self.bn3 = nn.BatchNorm2d(32)\n",
    "\n",
    "#         # Number of Linear input connections depends on output of conv2d layers\n",
    "#         # and therefore the input image size, so compute it.\n",
    "#         def conv2d_size_out(size, kernel_size = 5, stride = 2):\n",
    "#             return (size - (kernel_size - 1) - 1) // stride  + 1\n",
    "\n",
    "#         convw = conv2d_size_out(conv2d_size_out(conv2d_size_out(img_w)))\n",
    "#         convh = conv2d_size_out(conv2d_size_out(conv2d_size_out(img_h)))\n",
    "#         linear_input_size = convw * convh * 32\n",
    "#         self.head = nn.Linear(linear_input_size, num_actions)\n",
    "\n",
    "#     # Called with either one element to determine next action, or a batch\n",
    "#     # during optimization. Returns tensor([[left0exp,right0exp]...]).\n",
    "#     def forward(self, x):\n",
    "#         x = x.to(device)\n",
    "#         x = F.relu(self.bn1(self.conv1(x)))\n",
    "#         x = F.relu(self.bn2(self.conv2(x)))\n",
    "#         x = F.relu(self.bn3(self.conv3(x)))\n",
    "#         return self.head(x.view(x.size(0), -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(nn.Module):\n",
    "    def __init__(self, pred_model, target_model):\n",
    "        super(Agent, self).__init__()\n",
    "        self.prediction_net = pred_model\n",
    "        self.target_net = target_model \n",
    "        \n",
    "    def epsilon_greedy(self, env, state, epsilon):\n",
    "        prob = random.random()\n",
    "        q_val = torch.zeros(1, 4).to(device)\n",
    "        action = env.action_space.sample() # pick action from action space\n",
    "        if prob < 1 - epsilon.val:\n",
    "            q_val = self.agent_policy(state=state, pred_model=True, grad=False) # retrieve best action, based off its action-value \n",
    "            action = torch.argmax(q_val, 1)\n",
    "        return action, (torch.max(q_val, dim=1)[0]).detach()\n",
    "        \n",
    "    def agent_policy(self, state, pred_model=True, grad=False):\n",
    "        q_val = None\n",
    "        # 0.1: preprocess state\n",
    "        preprocessed_state = preprocess_two(state) # preprocess(state)\n",
    "        preprocessed_state = preprocessed_state.to(device)\n",
    "        if pred_model:\n",
    "            if grad:\n",
    "                q_val = self.prediction_net(preprocessed_state)\n",
    "            else:\n",
    "                with torch.no_grad():\n",
    "                    q_val = self.prediction_net(preprocessed_state)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                q_val = self.target_net(preprocessed_state)\n",
    "        return q_val\n",
    "    \n",
    "    def copy_pred_to_target(self):\n",
    "        self.target_net.load_state_dict(self.prediction_net.state_dict())\n",
    "        self.target_net.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Epsilon():\n",
    "    def __init__(self, epsilon_start=1, p_init=0.9, p_end=0.05, learn_start = False, decay_rate=200, max_episodes=10000, max_steps=10000):\n",
    "        self.val = epsilon_start\n",
    "        self.p_init = p_init\n",
    "        self.p_end = p_end\n",
    "        self.learn_start = learn_start\n",
    "        self.decay = decay_rate\n",
    "        self.max_episodes = max_episodes\n",
    "        self.max_steps = max_steps \n",
    "\n",
    "    def linear_epsilon_decay(self, episode_num):\n",
    "        if self.val >= self.p_end :\n",
    "            epsilon_decay_rate = max(((self.max_episodes - episode_num) / self.max_episodes), 0)\n",
    "            self.val = ((self.p_init - self.p_end) * (epsilon_decay_rate)) + self.p_end\n",
    "        else: \n",
    "            self.val = self.p_end\n",
    "\n",
    "    def quad_epsilon_decay(self, episode_num):\n",
    "        if self.val >= self.p_end: \n",
    "            epsilon_decay_rate = max(math.exp(-1. * episode_num / self.decay), 0)\n",
    "            self.val = ((self.p_init - self.p_end) * (epsilon_decay_rate)) + self.p_end\n",
    "        else: \n",
    "            self.val = self.p_end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(state):\n",
    "    \n",
    "    # convert state to numpy array and then to torch tensor\n",
    "    frame = torch.from_numpy(np.array(state).astype(np.float32))\n",
    "    \n",
    "    # reshape so that grayscaling is possible\n",
    "    reshaped_frame = frame.reshape(4, 3, 210, 160)\n",
    "    \n",
    "    # grayscale image\n",
    "    gray_frame = T.Grayscale()(reshaped_frame)\n",
    "    \n",
    "    # reshape image so network can process it\n",
    "    reshaped_gray_frame = gray_frame.reshape(1, 4, 210, 160)\n",
    "    \n",
    "    # downscale image to 84x84\n",
    "    small_gray_frame = T.Resize((84, 84))(reshaped_gray_frame)\n",
    "    \n",
    "    return small_gray_frame\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_two(state):\n",
    "    convert_arr = None\n",
    "    if len(state) > 4:\n",
    "        # add additional dimension to numpy array, so we can add batch then lower dimensionality\n",
    "        convert_arr = np.squeeze(np.array(np.expand_dims(state, 0)).astype(np.float32), axis=0)\n",
    "    else: \n",
    "        # add additional dimension to numpy array\n",
    "        convert_arr = np.array(np.expand_dims(state, 0)).astype(np.float32)\n",
    "    \n",
    "    # convert state to numpy array and then to torch tensor\n",
    "    return torch.from_numpy(convert_arr)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://docs.cleanrl.dev/rl-algorithms/dqn/\n",
    "\n",
    "def config_tune() -> None:\n",
    "    cfg = {\n",
    "        'method': 'bayes', #grid, random\n",
    "        'metric': {\n",
    "            'name': 'Mean Episodic Reward',\n",
    "            'goal': 'maximize' #minimize, maximize  \n",
    "        },\n",
    "        'parameters': {\n",
    "            'lr': {\n",
    "                'value': 1e-4    \n",
    "            },\n",
    "            'replay_buffer_size': {\n",
    "                'value': 1000000    \n",
    "            },\n",
    "            'gamma': {\n",
    "                'value': 0.99    \n",
    "            },\n",
    "            'target_freq': {'distribution': 'int_uniform',\n",
    "                                      'min': 1,\n",
    "                                      'max': 10000},\n",
    "            'p_end': {\n",
    "                'value': 0.01\n",
    "            },\n",
    "            'p_init': {\n",
    "                'value': 0.999\n",
    "            },\n",
    "            'epsilon_decay_rate': {'distribution': 'int_uniform',\n",
    "                                      'min': 1,\n",
    "                                      'max': 10000},\n",
    "            'epsilon_policy': {\n",
    "                'values': ['linear', 'quad']\n",
    "            },\n",
    "            'learning_start': {\n",
    "                'value': 80000    \n",
    "            },\n",
    "            'batch_size': {\n",
    "                'value': 32\n",
    "            },\n",
    "            'max_episodes': {\n",
    "                'value': 100000\n",
    "            },\n",
    "            'max_steps': {\n",
    "                'value': 100000\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vanilla_config() -> None:\n",
    "    cfg = {\n",
    "        'batch_size': 32,\n",
    "        'gamma': 0.99, \n",
    "        'lr': 1e-4, \n",
    "        'max_episodes': 100000,\n",
    "        'max_steps': 100000, \n",
    "        'p_end': 0.01,\n",
    "        'p_init': 0.999,\n",
    "        'replay_buffer_size': 1000000,\n",
    "        'target_freq': 1000, \n",
    "        'epsilon_decay_rate': 400, # need to find\n",
    "        'epsilon_policy': 'linear',\n",
    "        'learning_start': 80000\n",
    "    }\n",
    "    \n",
    "    return cfg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Buidling Optimizers and Schedulers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_optimizer(model, optimizer_name='adam', learning_rate=0.01, weight_decay=0.01, momentum=0.9):\n",
    "    try:\n",
    "        optimizer = None\n",
    "        if optimizer_name == \"sgd\":\n",
    "            optimizer = optim.SGD(model.parameters(), \n",
    "                                  lr=learning_rate, \n",
    "                                  momentum=momentum)\n",
    "            \n",
    "        elif optimizer_name == \"adam\":\n",
    "            optimizer = optim.Adam(model.parameters(), \n",
    "                                   lr=learning_rate)\n",
    "               \n",
    "        return optimizer\n",
    "    except:\n",
    "        print(\"Error: Invalid optimizer specified.\")\n",
    "        sys.exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_scheduler(optimizer, sched_name='reduce_lr', patience=5, verbose=True):\n",
    "    try: \n",
    "        sched = None\n",
    "        if sched_name == \"reduce_lr\":\n",
    "            sched = optim.lr_scheduler.ReduceLROnPlateau(optimizer, \n",
    "                                                         mode='min', \n",
    "                                                         patience=patience, \n",
    "                                                         verbose=verbose)\n",
    "        elif sched_name == 'TODO':\n",
    "            pass\n",
    "            #TODO: add other scheduler\n",
    "            \n",
    "        return sched\n",
    "    except:\n",
    "        logger.error(\"Error: Invalid scheduler specified.\")\n",
    "        sys.exit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(replay_buffer, the_agent, loss_fn, optimizer, scheduler, gamma=0.95):\n",
    "    #1. retrieve (s, a, r, s') from mini_batch\n",
    "    states, actions, next_states, rewards, done = replay_buffer.get_rb_batch()\n",
    "    \n",
    "    actions = torch.tensor(actions, dtype = torch.long, device = device)\n",
    "    rewards = torch.tensor(rewards, dtype = torch.float, device = device).detach()\n",
    "    done = torch.tensor(done, dtype=torch.int32, device = device).detach()\n",
    "    \n",
    "    pred_q_val_matrix = the_agent.agent_policy(states, pred_model=True, grad=True)\n",
    "    # print(f\"action {actions}\")\n",
    "    pred_q_val = torch.gather(pred_q_val_matrix, 1, actions.unsqueeze(1)).squeeze(1)\n",
    "    # print(f\"pred_q_val {pred_q_val}\")\n",
    "    target_q_val_matrix = the_agent.agent_policy(next_states.detach(), pred_model=False, grad=False).detach()\n",
    "    # print(f\"target_q_val_matrix {target_q_val_matrix}\")\n",
    "    target_q_val = torch.max(target_q_val_matrix, dim=1)[0]\n",
    "    # print(f\"target_q_val {target_q_val}\")\n",
    "    #zero_or_one = torch.ones(done.shape, dtype=torch.float, device = device) - done\n",
    "    y_j = rewards + ((gamma * target_q_val) * (1 - done))\n",
    "    # print(f\"y_j {y_j}\")\n",
    "    loss = loss_fn(pred_q_val, y_j.detach())\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Episode Specific Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode(env, the_agent, replay_buffer, epsilon, gamma, loss_fn, optimizer, scheduler):\n",
    "    step_count = 0\n",
    "    cumulative_reward = 0\n",
    "    cumulative_loss = 0\n",
    "    cumulative_q_val = 0\n",
    "    \n",
    "    # 0. get initial state, s_{0}\n",
    "    state = env.reset(seed=42)\n",
    "    \n",
    "    while True:\n",
    "        #1. either explore or exploit\n",
    "        action, action_q_val = the_agent.epsilon_greedy(env=env, \n",
    "                                          state=state, \n",
    "                                          epsilon=epsilon)\n",
    "        \n",
    "        #2. pass action to environment\n",
    "        (next_state, reward, done, info) = env.step(action)\n",
    "        \n",
    "        #3. add transition (s, a, s', r) to replay buffer\n",
    "        replay_buffer.add_to_rb((state, action, next_state, reward, done))\n",
    "        \n",
    "        #4. if replay buffer is full, sample mini batch and update model\n",
    "        if len(replay_buffer.rb) > replay_buffer.mini_batch_size and not epsilon.val <= 0.000001 and epsilon.learn_start and step_count % 4 == 0:\n",
    "            loss = train(replay_buffer, the_agent, loss_fn, optimizer, scheduler, gamma)\n",
    "            cumulative_loss += loss\n",
    "            \n",
    "        \n",
    "        cumulative_reward += reward\n",
    "        cumulative_q_val += action_q_val\n",
    "        state = next_state\n",
    "        \n",
    "        #5. check max number of time steps has been reached or if game is complete\n",
    "        if step_count >= epsilon.max_steps or done:\n",
    "            step_count += 1\n",
    "            return cumulative_loss, cumulative_reward, cumulative_q_val, step_count, info\n",
    "        \n",
    "        step_count += 1\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Episodic Loop  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run():\n",
    "    cfg = vanilla_config()\n",
    "    with wandb.init(project=\"BeeGass-Agents\", entity=\"beegass\", config=cfg, monitor_gym=True, mode=\"online\"):\n",
    "        config = wandb.config\n",
    "        \n",
    "        # initialize gym environment \"ALE/Pong-v5\"\n",
    "        env = make_env(env_name='ALE/Breakout-v5', seed=42)\n",
    "        #env = make_env(env_name='ALE/Pong-v5', seed=42)\n",
    "        \n",
    "        # set values for epsilon \n",
    "        epsilon = Epsilon(epsilon_start=1, \n",
    "                          p_init=config.p_init, \n",
    "                          p_end=config.p_end,\n",
    "                          learn_start = False,\n",
    "                          decay_rate=config.epsilon_decay_rate, \n",
    "                          max_episodes=config.max_episodes, \n",
    "                          max_steps=config.max_steps)\n",
    "        \n",
    "        eps_policy = config.epsilon_policy\n",
    "        \n",
    "        # initialize prediction network\n",
    "        #pred_net = Deep_Q_Network(environment.le_env.action_space.n).to(device)\n",
    "        pred_net = DQN(4, 6).to(device)\n",
    "        target_net = DQN(4, 6).to(device)\n",
    "        \n",
    "        # initialize agent that contains both prediction network and target network\n",
    "        the_agent = Agent(pred_model=pred_net, target_model=target_net)\n",
    "        the_agent.copy_pred_to_target()\n",
    "        \n",
    "        # define loss function\n",
    "        loss_fn = nn.SmoothL1Loss() #nn.HuberLoss(reduction='mean', delta=config.delta)\n",
    "        \n",
    "        # define optimizer\n",
    "        optimizer = build_optimizer(model=the_agent.prediction_net, \n",
    "                                    optimizer_name='adam', \n",
    "                                    learning_rate=config.lr)\n",
    "        \n",
    "        # define scheduler\n",
    "        scheduler = build_scheduler(optimizer, \n",
    "                                    sched_name='reduce_lr', \n",
    "                                    patience=5, \n",
    "                                    verbose=True)\n",
    "        \n",
    "        # initialize replay buffer\n",
    "        replay_buffer = Replay_Buffer(capacity=config.replay_buffer_size, mini_batch_size=config.batch_size)\n",
    "        \n",
    "        episode_cumulative_reward = 0\n",
    "        episode_cumulative_loss = 0 \n",
    "        episode_cumulative_q_val = 0\n",
    "        total_steps = 0\n",
    "        \n",
    "        for e in range(epsilon.max_episodes):\n",
    "            \n",
    "            if (episode_cumulative_loss/(e+1)) >= 1000:\n",
    "                print(\"EARLY STOPPING BECAUSE OF HIGH LOSS\")\n",
    "                continue \n",
    "            \n",
    "            if epsilon.val < epsilon.p_end:\n",
    "                print(\"EARLY STOPPING BECAUSE OF EPSILON\")\n",
    "                continue\n",
    "            \n",
    "            # 1. iterate over steps in episode\n",
    "            cumulative_loss, cumulative_reward, cumulative_q_val, step_count, episode_info  = run_episode(env=env, \n",
    "                                                                                                          the_agent=the_agent,\n",
    "                                                                                                          replay_buffer=replay_buffer, \n",
    "                                                                                                          epsilon=epsilon,\n",
    "                                                                                                          gamma=config.gamma,\n",
    "                                                                                                          loss_fn=loss_fn,\n",
    "                                                                                                          optimizer=optimizer, \n",
    "                                                                                                          scheduler=scheduler)\n",
    "            \n",
    "            env.close()\n",
    "            \n",
    "            # 3. decay epsilon\n",
    "            # epsilon = config.decay_rate * epsilon\n",
    "            \n",
    "            if eps_policy == \"linear\":\n",
    "                epsilon.linear_epsilon_decay(e+1)\n",
    "            else:\n",
    "                epsilon.quad_epsilon_decay(e+1)\n",
    "                \n",
    "            if e % config.target_freq == 0:\n",
    "                the_agent.copy_pred_to_target()\n",
    "            \n",
    "            if config.learning_start >= total_steps:\n",
    "                epsilon.learn_start = True\n",
    "            \n",
    "            if e+1 >= 5:\n",
    "                episode_cumulative_reward += cumulative_reward\n",
    "                episode_cumulative_loss += cumulative_loss\n",
    "                episode_cumulative_q_val += cumulative_q_val\n",
    "                total_steps += step_count\n",
    "                \n",
    "                wandb.log({\"episode\": e, \"Episode Info\": episode_info}, step=e)\n",
    "                wandb.log({\"episode\": e, \"Mean Episodic Action Value \": episode_cumulative_q_val/(e+1)}, step=e)\n",
    "                wandb.log({\"episode\": e, \"Mean Episodic Reward\": (episode_cumulative_reward/(e+1))}, step=e)\n",
    "                wandb.log({\"episode\": e, \"Reward Per Episode\": cumulative_reward}, step=e)\n",
    "                wandb.log({\"episode\": e, \"Step Count\": step_count}, step=e)\n",
    "                wandb.log({\"episode\": e, \"Total Step Count\": total_steps}, step=e)\n",
    "                wandb.log({\"episode\": e, \"Loss Per Episode\": cumulative_loss}, step=e)\n",
    "                wandb.log({\"episode\": e, \"Mean Episodic Loss\": episode_cumulative_loss/(e+1)}, step=e)\n",
    "                wandb.log({\"episode\": e, \"Epsilon\": epsilon.val}, step=e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "wandb version 0.12.16 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.14"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/beegass/Documents/Coding/Agents/pytorch-agent/dqn/wandb/run-20220509_021533-r4qi8nha</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/beegass/BeeGass-Agents/runs/r4qi8nha\" target=\"_blank\">cosmic-haze-232</a></strong> to <a href=\"https://wandb.ai/beegass/BeeGass-Agents\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A.L.E: Arcade Learning Environment (version 0.7.4+069f8bd)\n",
      "[Powered by Stella]\n",
      "/home/beegass/.virtualenvs/dl_1/lib/python3.8/site-packages/gym/utils/seeding.py:138: DeprecationWarning: \u001b[33mWARN: Function `hash_seed(seed, max_bytes)` is marked as deprecated and will be removed in the future. \u001b[0m\n",
      "  deprecation(\n",
      "/home/beegass/.virtualenvs/dl_1/lib/python3.8/site-packages/gym/utils/seeding.py:175: DeprecationWarning: \u001b[33mWARN: Function `_bigint_from_bytes(bytes)` is marked as deprecated and will be removed in the future. \u001b[0m\n",
      "  deprecation(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:red\">(failed 1).</strong> Press Control-C to abort syncing."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f76db0090274450b60f619a8101fb16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epsilon</td><td>███▇▇▇▇▇▇▆▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▁▁▁</td></tr><tr><td>Loss Per Episode</td><td>▄▄▅▄▄▄▄▃▃▂▂▂▅▅▁▂▂▇▄▃▁▅▅▂▂█▃▃▄▆▂▂▂▂▃▁▂▅▁▂</td></tr><tr><td>Mean Episodic Action Value </td><td>▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>Mean Episodic Loss</td><td>▇███▇▇▆▆▆▆▆▅▅▅▅▄▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁</td></tr><tr><td>Mean Episodic Reward</td><td>▆▁▅▇▇▅▄▅▅▇▆▆▇▆▇▆▆▆▆▆▇▆▇▆▆▇▆▇▇▇▇██████▇██</td></tr><tr><td>Reward Per Episode</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁██▁▁██▁▁█▁▁██▁▁▁▁▁▁▁█▁▁</td></tr><tr><td>Step Count</td><td>▁▂▂▁▂▂▃▂▁▁▁▂▁▅▁▁▁▆▄▁▂▅▇▁▃█▂▂▆▆▂▁▁▁▂▁▁▆▁▁</td></tr><tr><td>Total Step Count</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>episode</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epsilon</td><td>0.95595</td></tr><tr><td>Loss Per Episode</td><td>0.25793</td></tr><tr><td>Mean Episodic Action Value </td><td>0.96705</td></tr><tr><td>Mean Episodic Loss</td><td>0.12651</td></tr><tr><td>Mean Episodic Reward</td><td>0.20032</td></tr><tr><td>Reward Per Episode</td><td>2.0</td></tr><tr><td>Step Count</td><td>101</td></tr><tr><td>Total Step Count</td><td>150974</td></tr><tr><td>episode</td><td>4352</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">cosmic-haze-232</strong>: <a href=\"https://wandb.ai/beegass/BeeGass-Agents/runs/r4qi8nha\" target=\"_blank\">https://wandb.ai/beegass/BeeGass-Agents/runs/r4qi8nha</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220509_021533-r4qi8nha/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_731636/766504495.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_731636/3903945065.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m()\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0;31m# 1. iterate over steps in episode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m             cumulative_loss, cumulative_reward, cumulative_q_val, step_count, episode_info  = run_episode(env=env, \n\u001b[0m\u001b[1;32m     62\u001b[0m                                                                                                           \u001b[0mthe_agent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mthe_agent\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m                                                                                                           \u001b[0mreplay_buffer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreplay_buffer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_731636/596268036.py\u001b[0m in \u001b[0;36mrun_episode\u001b[0;34m(env, the_agent, replay_buffer, epsilon, gamma, loss_fn, optimizer, scheduler)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;31m#2. pass action to environment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0;34m(\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;31m#3. add transition (s, a, s', r) to replay buffer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/dl_1/lib/python3.8/site-packages/gym/wrappers/frame_stack.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/dl_1/lib/python3.8/site-packages/stable_baselines3/common/atari_wrappers.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mGymStepReturn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m         \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwas_real_done\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0;31m# check current lives, make loss of life terminal,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/dl_1/lib/python3.8/site-packages/gym/core.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 327\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    328\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/dl_1/lib/python3.8/site-packages/gym/wrappers/record_episode_statistics.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mobservations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdones\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepisode_returns\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepisode_lengths\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/dl_1/lib/python3.8/site-packages/gym/core.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mActType\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mObsType\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 280\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    281\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mObsType\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mObsType\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/dl_1/lib/python3.8/site-packages/gym/wrappers/atari_preprocessing.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframe_skip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m             \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m             \u001b[0mR\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgame_over\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/dl_1/lib/python3.8/site-packages/gym/wrappers/time_limit.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_max_episode_steps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/dl_1/lib/python3.8/site-packages/gym/wrappers/order_enforcing.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_reset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Cannot call env.step() before calling reset()\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/dl_1/lib/python3.8/site-packages/gym/envs/atari/environment.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action_ind)\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframeskip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m             \u001b[0mreward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0male\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_obs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterminal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: f0acawum\n",
      "Sweep URL: https://wandb.ai/beegass/dqn-sweeps/sweeps/f0acawum\n"
     ]
    }
   ],
   "source": [
    "sweep_id = wandb.sweep(config_tune(), project=\"dqn-sweeps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: wgleuab2 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepsilon_decay_rate: 1725\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepsilon_policy: linear\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tgamma: 0.99\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_start: 80000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.0001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmax_episodes: 100000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmax_steps: 100000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tp_end: 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tp_init: 0.999\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \treplay_buffer_size: 1000000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttarget_freq: 8712\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg entity when running a sweep.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.12.16 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.14"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/beegass/Documents/Coding/Agents/pytorch-agent/dqn/wandb/run-20220509_013354-wgleuab2</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/beegass/dqn-sweeps/runs/wgleuab2\" target=\"_blank\">smart-sweep-1</a></strong> to <a href=\"https://wandb.ai/beegass/dqn-sweeps\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/beegass/dqn-sweeps/sweeps/f0acawum\" target=\"_blank\">https://wandb.ai/beegass/dqn-sweeps/sweeps/f0acawum</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A.L.E: Arcade Learning Environment (version 0.7.4+069f8bd)\n",
      "[Powered by Stella]\n",
      "/home/beegass/.virtualenvs/dl_1/lib/python3.8/site-packages/gym/utils/seeding.py:138: DeprecationWarning: \u001b[33mWARN: Function `hash_seed(seed, max_bytes)` is marked as deprecated and will be removed in the future. \u001b[0m\n",
      "  deprecation(\n",
      "/home/beegass/.virtualenvs/dl_1/lib/python3.8/site-packages/gym/utils/seeding.py:175: DeprecationWarning: \u001b[33mWARN: Function `_bigint_from_bytes(bytes)` is marked as deprecated and will be removed in the future. \u001b[0m\n",
      "  deprecation(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:red\">(failed 1).</strong> Press Control-C to abort syncing."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d38c15c0a2c4e9c91d9ead74a4a626e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epsilon</td><td>███▇▇▇▇▇▇▆▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▁▁▁</td></tr><tr><td>Loss Per Episode</td><td>▆▄▅▄▆▃█▃▂▂▆▅▃▂▆▂▄▄▅▃▄▁▂▃▅▆▂▄▃▄▂▄▄▆▅▃▁▃▃▄</td></tr><tr><td>Mean Episodic Action Value </td><td>▁▁▁▂▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>Mean Episodic Loss</td><td>█▇▆▆▆▅▅▅▄▄▄▄▄▄▃▃▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁</td></tr><tr><td>Mean Episodic Reward</td><td>▁▁▃▆▄▅▅▆▆▆▆▆▆▇▆▆▆▆▆▆▆▇▇▇▇███████████▇██▇</td></tr><tr><td>Reward Per Episode</td><td>▁▁▁▁▁▁█▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁██▁▁▁▁▁</td></tr><tr><td>Step Count</td><td>▃▁▁▂▃▃▇▁▁▁▇▂▁▂▂▂▂▂▂▁▁▁▁▂▄█▂▁▁▃▁▁▂▇▆▁▁▁▂▅</td></tr><tr><td>Total Step Count</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>episode</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epsilon</td><td>0.89234</td></tr><tr><td>Loss Per Episode</td><td>0.16781</td></tr><tr><td>Mean Episodic Action Value </td><td>2.70111</td></tr><tr><td>Mean Episodic Loss</td><td>0.1927</td></tr><tr><td>Mean Episodic Reward</td><td>0.20241</td></tr><tr><td>Reward Per Episode</td><td>0.0</td></tr><tr><td>Step Count</td><td>36</td></tr><tr><td>Total Step Count</td><td>375049</td></tr><tr><td>episode</td><td>10784</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">smart-sweep-1</strong>: <a href=\"https://wandb.ai/beegass/dqn-sweeps/runs/wgleuab2\" target=\"_blank\">https://wandb.ai/beegass/dqn-sweeps/runs/wgleuab2</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220509_013354-wgleuab2/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run wgleuab2 errored: IndexError('list index out of range')\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run wgleuab2 errored: IndexError('list index out of range')\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 5cnyz8rr with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepsilon_decay_rate: 309\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepsilon_policy: linear\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tgamma: 0.99\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_start: 80000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.0001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmax_episodes: 100000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmax_steps: 100000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tp_end: 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tp_init: 0.999\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \treplay_buffer_size: 1000000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttarget_freq: 424\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg entity when running a sweep.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.12.16 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.14"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/beegass/Documents/Coding/Agents/pytorch-agent/dqn/wandb/run-20220509_015410-5cnyz8rr</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/beegass/dqn-sweeps/runs/5cnyz8rr\" target=\"_blank\">smart-sweep-2</a></strong> to <a href=\"https://wandb.ai/beegass/dqn-sweeps\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/beegass/dqn-sweeps/sweeps/f0acawum\" target=\"_blank\">https://wandb.ai/beegass/dqn-sweeps/sweeps/f0acawum</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/beegass/.virtualenvs/dl_1/lib/python3.8/site-packages/gym/utils/seeding.py:138: DeprecationWarning: \u001b[33mWARN: Function `hash_seed(seed, max_bytes)` is marked as deprecated and will be removed in the future. \u001b[0m\n",
      "  deprecation(\n",
      "/home/beegass/.virtualenvs/dl_1/lib/python3.8/site-packages/gym/utils/seeding.py:175: DeprecationWarning: \u001b[33mWARN: Function `_bigint_from_bytes(bytes)` is marked as deprecated and will be removed in the future. \u001b[0m\n",
      "  deprecation(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:red\">(failed 1).</strong> Press Control-C to abort syncing."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "470e68c4a42d40a394031806196c80eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epsilon</td><td>████▇▇▇▇▇▆▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▁▁▁</td></tr><tr><td>Loss Per Episode</td><td>▂▂▂▂▂█▂▃▂▁▂▂▂▂▁▂▂▂▁▂▁▁▂▁▁▁▄▂▁▂▁▂▂▁▁▁▂▂▁▃</td></tr><tr><td>Mean Episodic Action Value </td><td>▁▁▂▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>Mean Episodic Loss</td><td>█▆▆▅▅▅▅▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Mean Episodic Reward</td><td>█▂▂▂▁▃▂▃▃▃▃▃▃▃▃▂▃▃▃▃▃▄▃▃▃▃▃▃▃▃▃▃▃▄▄▄▄▄▄▄</td></tr><tr><td>Reward Per Episode</td><td>▁▁▁▁▁█▁▅▁▁▁▅▁▁▁▁▁▁▁▁▁▁▁▁▁▁▅▁▁▁▁▁▁▁▁▁▁▁▁▅</td></tr><tr><td>Step Count</td><td>▁▁▁▁▁█▁▄▁▁▁▃▁▁▁▁▁▁▁▁▁▁▁▁▂▁▅▂▁▂▁▂▁▂▁▁▁▂▁▅</td></tr><tr><td>Total Step Count</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>episode</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epsilon</td><td>0.97096</td></tr><tr><td>Loss Per Episode</td><td>0.1509</td></tr><tr><td>Mean Episodic Action Value </td><td>0.59058</td></tr><tr><td>Mean Episodic Loss</td><td>0.10054</td></tr><tr><td>Mean Episodic Reward</td><td>0.19965</td></tr><tr><td>Reward Per Episode</td><td>1.0</td></tr><tr><td>Step Count</td><td>53</td></tr><tr><td>Total Step Count</td><td>98355</td></tr><tr><td>episode</td><td>2834</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">smart-sweep-2</strong>: <a href=\"https://wandb.ai/beegass/dqn-sweeps/runs/5cnyz8rr\" target=\"_blank\">https://wandb.ai/beegass/dqn-sweeps/runs/5cnyz8rr</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220509_015410-5cnyz8rr/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run 5cnyz8rr errored: IndexError('list index out of range')\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run 5cnyz8rr errored: IndexError('list index out of range')\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: gk61zxmj with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepsilon_decay_rate: 9537\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepsilon_policy: quad\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tgamma: 0.99\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_start: 80000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.0001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmax_episodes: 100000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmax_steps: 100000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tp_end: 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tp_init: 0.999\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \treplay_buffer_size: 1000000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttarget_freq: 1932\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg entity when running a sweep.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.12.16 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.14"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/beegass/Documents/Coding/Agents/pytorch-agent/dqn/wandb/run-20220509_015932-gk61zxmj</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/beegass/dqn-sweeps/runs/gk61zxmj\" target=\"_blank\">morning-sweep-3</a></strong> to <a href=\"https://wandb.ai/beegass/dqn-sweeps\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/beegass/dqn-sweeps/sweeps/f0acawum\" target=\"_blank\">https://wandb.ai/beegass/dqn-sweeps/sweeps/f0acawum</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/beegass/.virtualenvs/dl_1/lib/python3.8/site-packages/gym/utils/seeding.py:138: DeprecationWarning: \u001b[33mWARN: Function `hash_seed(seed, max_bytes)` is marked as deprecated and will be removed in the future. \u001b[0m\n",
      "  deprecation(\n",
      "/home/beegass/.virtualenvs/dl_1/lib/python3.8/site-packages/gym/utils/seeding.py:175: DeprecationWarning: \u001b[33mWARN: Function `_bigint_from_bytes(bytes)` is marked as deprecated and will be removed in the future. \u001b[0m\n",
      "  deprecation(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:red\">(failed 1).</strong> Press Control-C to abort syncing."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca873094d8a0436593699a158b51894d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epsilon</td><td>███▇▇▇▇▇▆▆▆▆▆▅▅▅▅▅▅▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▁▁▁▁</td></tr><tr><td>Loss Per Episode</td><td>▇▂█▁▁▂▇▂▃▂▃▃▁▁▂▂▂▂▁▁▂▁▂▁▂▂▃▂▁▄▂▁▂▂▁▃▂▂▁▁</td></tr><tr><td>Mean Episodic Action Value </td><td>▁▁▁▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>Mean Episodic Loss</td><td>█▆▆▅▄▄▄▃▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁</td></tr><tr><td>Mean Episodic Reward</td><td>█▁▅▃▂▃▄▄▄▃▄▄▄▄▃▄▄▄▄▃▃▃▃▃▃▃▃▃▃▃▂▃▂▂▂▂▂▂▂▂</td></tr><tr><td>Reward Per Episode</td><td>▅▁█▁▁▁█▁▁▁▁▅▁▁▁▁▁▁▁▁▁▁▁▁▁▁▅▅▁█▁▁▁▁▁▅▁▁▁▁</td></tr><tr><td>Step Count</td><td>▄▁█▁▁▁▅▁▁▁▂▃▂▁▁▁▁▁▁▁▂▂▁▁▁▁▅▃▁▆▂▁▂▁▁▃▂▁▁▁</td></tr><tr><td>Total Step Count</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>episode</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epsilon</td><td>0.73816</td></tr><tr><td>Loss Per Episode</td><td>0.04503</td></tr><tr><td>Mean Episodic Action Value </td><td>3.94488</td></tr><tr><td>Mean Episodic Loss</td><td>0.0706</td></tr><tr><td>Mean Episodic Reward</td><td>0.20582</td></tr><tr><td>Reward Per Episode</td><td>0.0</td></tr><tr><td>Step Count</td><td>26</td></tr><tr><td>Total Step Count</td><td>101579</td></tr><tr><td>episode</td><td>2919</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">morning-sweep-3</strong>: <a href=\"https://wandb.ai/beegass/dqn-sweeps/runs/gk61zxmj\" target=\"_blank\">https://wandb.ai/beegass/dqn-sweeps/runs/gk61zxmj</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220509_015932-gk61zxmj/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run gk61zxmj errored: IndexError('list index out of range')\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run gk61zxmj errored: IndexError('list index out of range')\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 47dtxa81 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepsilon_decay_rate: 6654\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepsilon_policy: linear\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tgamma: 0.99\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_start: 80000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.0001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmax_episodes: 100000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmax_steps: 100000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tp_end: 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tp_init: 0.999\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \treplay_buffer_size: 1000000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttarget_freq: 2331\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg entity when running a sweep.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.12.16 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.14"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/beegass/Documents/Coding/Agents/pytorch-agent/dqn/wandb/run-20220509_020519-47dtxa81</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/beegass/dqn-sweeps/runs/47dtxa81\" target=\"_blank\">ancient-sweep-4</a></strong> to <a href=\"https://wandb.ai/beegass/dqn-sweeps\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/beegass/dqn-sweeps/sweeps/f0acawum\" target=\"_blank\">https://wandb.ai/beegass/dqn-sweeps/sweeps/f0acawum</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/beegass/.virtualenvs/dl_1/lib/python3.8/site-packages/gym/utils/seeding.py:138: DeprecationWarning: \u001b[33mWARN: Function `hash_seed(seed, max_bytes)` is marked as deprecated and will be removed in the future. \u001b[0m\n",
      "  deprecation(\n",
      "/home/beegass/.virtualenvs/dl_1/lib/python3.8/site-packages/gym/utils/seeding.py:175: DeprecationWarning: \u001b[33mWARN: Function `_bigint_from_bytes(bytes)` is marked as deprecated and will be removed in the future. \u001b[0m\n",
      "  deprecation(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Ctrl + C detected. Stopping sweep.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:red\">(failed 1).</strong> Press Control-C to abort syncing."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6eb7e9cd8d444d95853e56d42e92223b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process wandb_internal:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/beegass/.virtualenvs/dl_1/lib/python3.8/site-packages/wandb/sdk/internal/internal.py\", line 162, in wandb_internal\n",
      "    thread.join()\n",
      "  File \"/usr/lib/python3.8/threading.py\", line 1011, in join\n",
      "    self._wait_for_tstate_lock()\n",
      "  File \"/usr/lib/python3.8/threading.py\", line 1027, in _wait_for_tstate_lock\n",
      "    elif lock.acquire(block, timeout):\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epsilon</td><td>████▇▇▇▇▇▆▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▁▁▁</td></tr><tr><td>Loss Per Episode</td><td>▂▅▃▂▃▂▂▃█▃▂▁▂▁▂▂▂▂▂▁▂▁▃▂▁▂▁▂▄▃▅▃▂▁▂▂▂▁▂▂</td></tr><tr><td>Mean Episodic Action Value </td><td>▁▁▂▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>Mean Episodic Loss</td><td>▄▃▅▆▆█▇▆▅▆▆▆▅▅▅▆▆▆▆▇▇▇▆▆▆▅▅▅▄▄▃▃▃▃▂▂▂▂▁▁</td></tr><tr><td>Mean Episodic Reward</td><td>█▁▁▁▃▅▄▃▃▃▄▃▂▃▃▄▄▄▅▅▆▆▆▆▆▆▇▆▆▆▆▆▇▇▇▇▇▆▇▇</td></tr><tr><td>Reward Per Episode</td><td>▁█▁▁▁▁▁▁█▅▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁█▅▁▁▁▁▁▁▁▁</td></tr><tr><td>Step Count</td><td>▁▇▁▂▁▁▁▂█▃▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▂▆▂▇▅▁▁▁▂▂▁▁▁</td></tr><tr><td>Total Step Count</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>episode</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epsilon</td><td>0.9583</td></tr><tr><td>Loss Per Episode</td><td>0.33619</td></tr><tr><td>Mean Episodic Action Value </td><td>2.11644</td></tr><tr><td>Mean Episodic Loss</td><td>0.5891</td></tr><tr><td>Mean Episodic Reward</td><td>0.18104</td></tr><tr><td>Reward Per Episode</td><td>0.0</td></tr><tr><td>Step Count</td><td>30</td></tr><tr><td>Total Step Count</td><td>139742</td></tr><tr><td>episode</td><td>4114</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">ancient-sweep-4</strong>: <a href=\"https://wandb.ai/beegass/dqn-sweeps/runs/47dtxa81\" target=\"_blank\">https://wandb.ai/beegass/dqn-sweeps/runs/47dtxa81</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220509_020519-47dtxa81/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.agent(sweep_id, run, count=12)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e235bd63d0dd74684ecb378f81599b59bf279545966289267647744a13ab7ca1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('dl_1': virtualenv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
